diff --git a/Makefile b/Makefile
index 1b4bec8e6..b8ccefa33 100644
--- a/Makefile
+++ b/Makefile
@@ -1062,10 +1062,10 @@ ocamldoc.opt: ocamlc.opt ocamlyacc ocamllex
 
 # OCamltest
 ocamltest: ocamlc ocamlyacc ocamllex
-	$(MAKE) -C ocamltest
+#	$(MAKE) -C ocamltest
 
 ocamltest.opt: ocamlc.opt ocamlyacc ocamllex
-	$(MAKE) -C ocamltest ocamltest.opt$(EXE)
+#	$(MAKE) -C ocamltest ocamltest.opt$(EXE)
 
 partialclean::
 	$(MAKE) -C ocamltest clean
diff --git a/asmcomp/arm/arch.ml b/asmcomp/arm/arch.ml
index becfff389..49f8b1054 100644
--- a/asmcomp/arm/arch.ml
+++ b/asmcomp/arm/arch.ml
@@ -18,14 +18,16 @@
 
 open Format
 
-type abi = EABI | EABI_HF
-type arch = ARMv4 | ARMv5 | ARMv5TE | ARMv6 | ARMv6T2 | ARMv7 | ARMv8
+(* EABI is actually GNU EABI *)
+type abi = EABI | EABI_HF | EABI_APPLE
+type arch = ARMv4 | ARMv5 | ARMv5TE | ARMv6 | ARMv6T2 | ARMv7 | ARMv7s | ARMv8
 type fpu = Soft | VFPv2 | VFPv3_D16 | VFPv3
 
 let abi =
   match Config.system with
     "linux_eabi" | "freebsd" -> EABI
   | "linux_eabihf" | "netbsd" -> EABI_HF
+  | "macosx" -> EABI_APPLE
   | _ -> assert false
 
 let string_of_arch = function
@@ -35,6 +37,7 @@ let string_of_arch = function
   | ARMv6   -> "armv6"
   | ARMv6T2 -> "armv6t2"
   | ARMv7   -> "armv7"
+  | ARMv7s  -> "armv7s"  (* = armv7 + div *)
   | ARMv8   -> "armv8"
 
 let string_of_fpu = function
@@ -45,22 +48,33 @@ let string_of_fpu = function
 
 (* Machine-specific command-line options *)
 
-let (arch, fpu, thumb) =
-  let (def_arch, def_fpu, def_thumb) =
+let (arch, fpu, thumb, supp_pic, supp_pie) =
+  let (def_arch, def_fpu, def_thumb, supp_pic, supp_pie) =
     begin match abi, Config.model with
-    (* Defaults for architecture, FPU and Thumb *)
-      EABI, "armv5"    -> ARMv5,   Soft,      false
-    | EABI, "armv5te"  -> ARMv5TE, Soft,      false
-    | EABI, "armv6"    -> ARMv6,   Soft,      false
-    | EABI, "armv6t2"  -> ARMv6T2, Soft,      false
-    | EABI, "armv7"    -> ARMv7,   Soft,      false
-    | EABI, "armv8"    -> ARMv8,   Soft,      false
-    | EABI, _          -> ARMv4,   Soft,      false
-    | EABI_HF, "armv6" -> ARMv6,   VFPv2,     false
-    | EABI_HF, "armv8" -> ARMv8,   VFPv3,     true
-    | EABI_HF, _       -> ARMv7,   VFPv3_D16, true
+    (* Defaults for architecture, FPU, Thumb, PIC and PIE *)
+      EABI,       "armv5"    -> ARMv5,   Soft,      false, true,  false
+    | EABI,       "armv5te"  -> ARMv5TE, Soft,      false, true,  false
+    | EABI,       "armv6"    -> ARMv6,   Soft,      false, true,  false
+    | EABI,       "armv6t2"  -> ARMv6T2, Soft,      false, true,  false
+    | EABI,       "armv7"    -> ARMv7,   Soft,      false, true,  false
+    | EABI,       "armv7s"   -> ARMv7,   Soft,      false, true,  false
+    | EABI,	  "armv8"    -> ARMv8,   Soft,      false, true,  false
+    | EABI,       _          -> ARMv4,   Soft,      false, true,  false
+    | EABI_HF,    "armv6t2"  -> ARMv6,   VFPv2,     true,  true,  false
+    | EABI_HF,    "armv7"    -> ARMv7,   VFPv3_D16, true,  true,  false
+    | EABI_HF,    "armv7s"   -> ARMv7s,  VFPv3_D16, true,  true,  false
+    | EABI_HF,    "armv8"    -> ARMv8,   VFPv3,     true,  true,  false
+    | EABI_HF,    _          -> ARMv6,   VFPv2,     false, true,  false
+    | EABI_APPLE, "armv7"    -> ARMv7,   VFPv3,     true,  false, true
+    | EABI_APPLE, "armv7s"   -> ARMv7s,  VFPv3,     true,  false, true
+    | EABI_APPLE, "armv8"    -> ARMv8,   VFPv3,     true,  false, true
+    | EABI_APPLE, _          -> ARMv6,   VFPv2,     false, false, false
     end in
-  (ref def_arch, ref def_fpu, ref def_thumb)
+  (ref def_arch, ref def_fpu, ref def_thumb, supp_pic, supp_pie)
+
+(* if pic/pie is supported the default is to enable it *)
+let pic_code = ref supp_pic
+let pie_code = ref supp_pie
 
 let farch spec =
   arch := begin match spec with
@@ -70,19 +84,31 @@ let farch spec =
            | "armv6"                       -> ARMv6
            | "armv6t2"                     -> ARMv6T2
            | "armv7"                       -> ARMv7
+           | "armv7s"                      -> ARMv7s
            | "armv8"                       -> ARMv8
            | spec -> raise (Arg.Bad ("wrong '-farch' option: " ^ spec))
   end
 
+let have_hf =
+  abi = EABI_HF || abi = EABI_APPLE
+
 let ffpu spec =
   fpu := begin match spec with
-            "soft" when abi <> EABI_HF     -> Soft
-          | "vfpv2" when abi = EABI_HF     -> VFPv2
-          | "vfpv3-d16" when abi = EABI_HF -> VFPv3_D16
-          | "vfpv3" when abi = EABI_HF     -> VFPv3
+            "soft" when not have_hf        -> Soft
+          | "vfpv2" when have_hf           -> VFPv2
+          | "vfpv3-d16" when have_hf       -> VFPv3_D16
+          | "vfpv3" when have_hf           -> VFPv3
           | spec -> raise (Arg.Bad ("wrong '-ffpu' option: " ^ spec))
   end
 
+let set_pic() =
+  if not supp_pic then raise(Arg.Bad "PIC not supported for this model");
+  pic_code := true
+
+let set_pie() =
+  if not supp_pie then raise(Arg.Bad "PIE not supported for this model");
+  pie_code := true
+
 let command_line_options =
   [ "-farch", Arg.String farch,
       "<arch>  Select the ARM target architecture"
@@ -90,9 +116,13 @@ let command_line_options =
     "-ffpu", Arg.String ffpu,
       "<fpu>  Select the floating-point hardware"
       ^ " (default: " ^ (string_of_fpu !fpu) ^ ")";
-    "-fPIC", Arg.Set Clflags.pic_code,
-      " Generate position-independent machine code";
-    "-fno-PIC", Arg.Clear Clflags.pic_code,
+    "-fPIC", Arg.Unit set_pic,
+      " Generate position-independent machine code for shared libs";
+    "-fno-PIC", Arg.Clear pic_code,
+      " Generate position-dependent machine code";
+    "-fPIE", Arg.Unit set_pie,
+      " Generate position-independent machine code for executables";
+    "-fno-PIE", Arg.Clear pie_code,
       " Generate position-dependent machine code";
     "-fthumb", Arg.Set thumb,
       " Enable Thumb/Thumb-2 code generation"
diff --git a/asmcomp/arm/emit.mlp b/asmcomp/arm/emit.mlp
index 182ccbdfd..290d16ff4 100644
--- a/asmcomp/arm/emit.mlp
+++ b/asmcomp/arm/emit.mlp
@@ -17,6 +17,8 @@
 
 (* Emission of ARM assembly code *)
 
+module StrMap = Map.Make(String)
+
 open Misc
 open Cmm
 open Arch
@@ -26,6 +28,9 @@ open Mach
 open Linearize
 open Emitaux
 
+let macosx =
+  Config.system = "macosx"
+
 (* Tradeoff between code size and code speed *)
 
 let fastcode_flag = ref true
@@ -33,20 +38,21 @@ let fastcode_flag = ref true
 (* Output a label *)
 
 let emit_label lbl =
-  emit_string ".L"; emit_int lbl
+  emit_string (if macosx then "L" else ".L"); emit_int lbl
 
 (* Symbols *)
 
 let emit_symbol s =
+  if macosx then emit_string "_";
   Emitaux.emit_symbol '$' s
 
 let emit_call s =
-  if !Clflags.dlcode || !Clflags.pic_code
+  if (!Clflags.dlcode || !Clflags.pic_code) && not macosx
   then `bl	{emit_symbol s}(PLT)`
   else `bl	{emit_symbol s}`
 
 let emit_jump s =
-  if !Clflags.dlcode || !Clflags.pic_code
+  if (!Clflags.dlcode || !Clflags.pic_code) && not macosx
   then `b	{emit_symbol s}(PLT)`
   else `b	{emit_symbol s}`
 
@@ -182,12 +188,13 @@ let name_for_int_operation = function
   | Isub  -> "subs"
   | Imul  -> "mul"
   | Imulh -> "smmul"
+  | Idiv  -> "sdiv"   (* only armv7s *)
   | Iand  -> "ands"
   | Ior   -> "orrs"
   | Ixor  -> "eors"
-  | Ilsl  -> "lsls"
-  | Ilsr  -> "lsrs"
-  | Iasr  -> "asrs"
+  | Ilsl  -> "lsls" (* NB. The T1 encoding only supports shifts from 1-32 *)
+  | Ilsr  -> "lsrs" (* NB. The T1 encoding only supports shifts from 1-32 *)
+  | Iasr  -> "asrs" (* NB. The T1 encoding only supports shifts from 1-32 *)
   | _ -> assert false
 
 let name_for_shift_operation = function
@@ -282,6 +289,8 @@ let float_literals = ref ([] : (int64 * label) list)
 let gotrel_literals = ref ([] : (label * label) list)
 (* Pending symbol literals *)
 let symbol_literals = ref ([] : (string * label) list)
+(* Symbol literals emitted at the end of the object *)
+let delayed_symbol_literals = ref (StrMap.empty : label StrMap.t)
 (* Total space (in words) occupied by pending literals *)
 let size_literals = ref 0
 
@@ -312,7 +321,16 @@ let symbol_literal s =
     symbol_literals := (s, lbl) :: !symbol_literals;
     lbl
 
-(* Emit all pending literals *)
+let delayed_symbol_literal s =
+  try
+    StrMap.find s !delayed_symbol_literals
+  with Not_found ->
+    let lbl = new_label() in
+    delayed_symbol_literals := StrMap.add s lbl !delayed_symbol_literals;
+    lbl
+
+
+(* Emit all pending literals, but not the delayed ones *)
 let emit_literals() =
   if !float_literals <> [] then begin
     `	.align	3\n`;
@@ -339,10 +357,60 @@ let emit_literals() =
   end;
   size_literals := 0
 
+
+let emit_delayed_symbol_literals() =
+  if !delayed_symbol_literals <> StrMap.empty then begin
+    assert macosx;
+    let direct, indirect =
+      List.partition
+        (fun (sym, _lbl) ->
+           Compilenv.symbol_in_current_unit sym
+        )
+        (StrMap.bindings !delayed_symbol_literals) in
+    (* references to symbols defined in this object can be resolved by the
+       assembler
+     *)
+    if direct <> [] then begin
+      `	.data\n`;
+      `	.align	2\n`;
+      List.iter
+        (fun (sym, lbl) ->
+           `{emit_label lbl}:	.word	{emit_symbol sym}\n`;
+        )
+        direct
+    end;
+    (* references to symbols defined in other objects need to go into the
+       section for indirect symbols, and will be resolved statically by ld.
+     *)
+    if indirect <> [] then begin
+      `	.non_lazy_symbol_pointer\n`;
+      `	.align	2\n`;
+      List.iter
+        (fun (sym, lbl) ->
+          `{emit_label lbl}:	.indirect_symbol	{emit_symbol sym}\n`;
+          `	.long 0\n`;
+        )
+        indirect;
+    end;
+    delayed_symbol_literals := StrMap.empty
+  end
+
+
 (* Emit code to load the address of a symbol *)
 
 let emit_load_symbol_addr dst s =
-  if !Clflags.pic_code then begin
+  if !pie_code then begin
+    assert macosx;
+    let offset = if !thumb then 4 else 8 in
+    let lbl_pic = new_label() in
+    let lbl_sym = delayed_symbol_literal s in
+    `	movw	{emit_reg dst}, #:lower16:({emit_label lbl_sym}-({emit_label lbl_pic}+{emit_int offset}))\n`;
+    `	movt	{emit_reg dst}, #:upper16:({emit_label lbl_sym}-({emit_label lbl_pic}+{emit_int offset}))\n`;
+    `{emit_label lbl_pic}:	add	{emit_reg dst}, pc\n`;
+    `	ldr	{emit_reg dst}, [{emit_reg dst}]\n`;
+    4
+  end else if !Clflags.pic_code then begin
+    assert (not macosx);
     let lbl_pic = new_label() in
     let lbl_got = gotrel_literal lbl_pic in
     let lbl_sym = symbol_literal s in
@@ -357,7 +425,7 @@ let emit_load_symbol_addr dst s =
     `{emit_label lbl_pic}:	add	{emit_reg tmp}, pc, {emit_reg tmp}\n`;
     `	ldr	{emit_reg dst}, [{emit_reg tmp}, {emit_reg dst}] @ {emit_symbol s}\n`;
     4
-  end else if !arch > ARMv6 && not !Clflags.dlcode && !fastcode_flag then begin
+  end else if !arch > ARMv6 && not !Clflags.dlcode && !fastcode_flag && not macosx then begin
     `	movw	{emit_reg dst}, #:lower16:{emit_symbol s}\n`;
     `	movt	{emit_reg dst}, #:upper16:{emit_symbol s}\n`;
     2
@@ -396,6 +464,84 @@ let emit_set_condition cmp rd =
     end
   end
 
+(* Load VFP immediate constant *)
+
+type vfp_imm_case =
+  | F_imm_none
+  | F_imm_binary of int
+  | F_imm_decimal of string
+
+let encode_vfp_immediate imm =
+  let sg = Int64.to_int (Int64.shift_right_logical imm 63) in
+  let ex = Int64.to_int (Int64.shift_right_logical imm 52) in
+  let ex = (ex land 0x7ff) - 1023 in
+  let mn = Int64.logand imm 0xfffffffffffffL in
+  if Int64.logand mn 0xffffffffffffL <> 0L || ex < -3 || ex > 4
+  then
+    F_imm_none
+  else begin
+      let mn = Int64.to_int (Int64.shift_right_logical mn 48) in
+      if mn land 0x0f <> mn then
+        F_imm_none
+      else
+        if macosx then
+          (* Apple needs vmov.f64 with a decimal immediate *)
+          F_imm_decimal (Printf.sprintf "%1.6e" (Int64.float_of_bits imm))
+        else
+          let ex = ((ex + 3) land 0x07) lxor 0x04 in
+          F_imm_binary ((sg lsl 7) lor (ex lsl 4) lor mn)
+    end
+
+(* Helpers for Apple EABI *)
+
+(* We pretended that we pass floats via d0 and d1. Actually, these values
+   are put into the pairs r0/r1 or r1/r2 or r2/r3, potentially mixed with
+   int values. We already ensured that we do not pass more than four
+   32 bit words via registers. Now move the values where iOS expects them.
+ *)
+
+let move_extcall_args i =
+  let l = ref [] in
+  let d = ref 0 in
+  Array.iter
+    (fun arg ->
+       match arg with
+         | { loc = Reg r } when r >= 100 ->
+              l := (r, !d) :: !l;
+              d := !d + 2
+         | { loc = Reg r } ->
+              l := (r, !d) :: !l;
+              incr d
+         | { loc = _ } ->
+              ()
+    )
+    i.arg;
+  List.iter
+    (fun (src, dst) ->
+       if src <> dst then (
+         let srcname = register_name src in
+         let dstname = register_name dst in
+         if src >= 100 then
+           let d2name = register_name (dst+1) in
+           `	vmov	{emit_string dstname}, {emit_string d2name}, {emit_string srcname}\n`
+         else
+           `	mov	{emit_string dstname}, {emit_string srcname}\n`
+       )
+    )
+    !l;
+  List.length !l
+
+let move_extcall_res i =
+  (* there can be at most one float result: move it from r0/r1 to d0 *)
+  if Array.length i.res > 0 then
+    match i.res.(0) with
+      | { loc = Reg r } when r = 100 ->
+           `	vmov	d0, r0, r1\n`; 1
+      | _ ->
+           0
+  else
+    0
+
 (* Output the assembly code for an instruction *)
 
 let emit_instr i =
@@ -407,15 +553,15 @@ let emit_instr i =
         if src.loc = dst.loc then 0 else begin
           begin match (src, dst) with
             {loc = Reg _; typ = Float}, {loc = Reg _} ->
-              `	fcpyd	{emit_reg dst}, {emit_reg src}\n`
+              `	vmov.f64	{emit_reg dst}, {emit_reg src}\n`
           | {loc = Reg _}, {loc = Reg _} ->
               `	mov	{emit_reg dst}, {emit_reg src}\n`
           | {loc = Reg _; typ = Float}, _ ->
-              `	fstd	{emit_reg src}, {emit_stack dst}\n`
+              `	vstr.64	{emit_reg src}, {emit_stack dst}\n`
           | {loc = Reg _}, _ ->
               `	str	{emit_reg src}, {emit_stack dst}\n`
           | {typ = Float}, _ ->
-              `	fldd	{emit_reg dst}, {emit_stack src}\n`
+              `	vldr.64	{emit_reg dst}, {emit_stack src}\n`
           | _ ->
               `	ldr	{emit_reg dst}, {emit_stack src}\n`
           end; 1
@@ -437,31 +583,17 @@ let emit_instr i =
         end
     | Lop(Iconst_float f) when !fpu = VFPv2 ->
         let lbl = float_literal f in
-        `	fldd	{emit_reg i.res.(0)}, {emit_label lbl}\n`;
+        `	vldr.64	{emit_reg i.res.(0)}, {emit_label lbl}\n`;
         1
     | Lop(Iconst_float f) ->
-        let encode imm =
-          let sg = Int64.to_int (Int64.shift_right_logical imm 63) in
-          let ex = Int64.to_int (Int64.shift_right_logical imm 52) in
-          let ex = (ex land 0x7ff) - 1023 in
-          let mn = Int64.logand imm 0xfffffffffffffL in
-          if Int64.logand mn 0xffffffffffffL <> 0L || ex < -3 || ex > 4
-          then
-            None
-          else begin
-            let mn = Int64.to_int (Int64.shift_right_logical mn 48) in
-            if mn land 0x0f <> mn then
-              None
-            else
-              let ex = ((ex + 3) land 0x07) lxor 0x04 in
-              Some((sg lsl 7) lor (ex lsl 4) lor mn)
-          end in
-        begin match encode f with
-          None ->
+        begin match encode_vfp_immediate f with
+          F_imm_none ->
             let lbl = float_literal f in
-            `	fldd	{emit_reg i.res.(0)}, {emit_label lbl}\n`
-        | Some imm8 ->
-            `	fconstd	{emit_reg i.res.(0)}, #{emit_int imm8}\n`
+            `	vldr.64	{emit_reg i.res.(0)}, {emit_label lbl}\n`
+        | F_imm_binary imm8 ->
+            `	vmov.f64	{emit_reg i.res.(0)}, #{emit_int imm8}\n`
+        | F_imm_decimal imm ->
+            `	vmov.f64	{emit_reg i.res.(0)}, #{emit_string imm}\n`
         end; 1
     | Lop(Iconst_symbol s) ->
         emit_load_symbol_addr i.res.(0) s
@@ -494,20 +626,25 @@ let emit_instr i =
           end
         end
     | Lop(Iextcall { func; alloc = false; }) ->
-        `	{emit_call func}\n`; 1
+        let n1 = if abi = EABI_APPLE then move_extcall_args i else 0 in
+        ` {emit_call func}\n`;
+        let n2 = if abi = EABI_APPLE then move_extcall_res i else 0 in
+        n1 + 1 + n2
     | Lop(Iextcall { func; alloc = true; label_after; }) ->
+        let n1 = if abi = EABI_APPLE then move_extcall_args i else 0 in
         let ninstr = emit_load_symbol_addr (phys_reg 7 (* r7 *)) func in
-        `	{emit_call "caml_c_call"}\n`;
+        ` {emit_call "caml_c_call"}\n`;
+        let n2 = if abi = EABI_APPLE then move_extcall_res i else 0 in
         `{record_frame i.live false i.dbg ~label:label_after}\n`;
-        1 + ninstr
+        n1 + 1 + ninstr + n2
     | Lop(Istackoffset n) ->
         assert (n mod 8 = 0);
         let ninstr = emit_stack_adjustment (-n) in
         stack_offset := !stack_offset + n;
         ninstr
     | Lop(Iload(Single, addr)) when !fpu >= VFPv2 ->
-        `	flds	s14, {emit_addressing addr i.arg 0}\n`;
-        `	fcvtds	{emit_reg i.res.(0)}, s14\n`; 2
+        `	vldr.32	s14, {emit_addressing addr i.arg 0}\n`;
+        `	vcvt.f64.f32	{emit_reg i.res.(0)}, s14\n`; 2
     | Lop(Iload((Double | Double_u), addr)) when !fpu = Soft ->
         (* Use LDM or LDRD if possible *)
         begin match i.res.(0), i.res.(1), addr with
@@ -536,12 +673,12 @@ let emit_instr i =
           | Sixteen_unsigned -> "ldrh"
           | Sixteen_signed -> "ldrsh"
           | Double
-          | Double_u -> "fldd"
+          | Double_u -> "vldr.64"
           | _ (* 32-bit quantities *) -> "ldr" in
         `	{emit_string instr}	{emit_reg r}, {emit_addressing addr i.arg 0}\n`; 1
     | Lop(Istore(Single, addr, _)) when !fpu >= VFPv2 ->
-        `	fcvtsd	s14, {emit_reg i.arg.(0)}\n`;
-        `	fsts	s14, {emit_addressing addr i.arg 1}\n`; 2
+        `	vcvt.f32.f64	s14, {emit_reg i.arg.(0)}\n`;
+        `	vstr.f32	s14, {emit_addressing addr i.arg 1}\n`; 2
     | Lop(Istore((Double | Double_u), addr, _)) when !fpu = Soft ->
         (* Use STM or STRD if possible *)
         begin match i.arg.(0), i.arg.(1), addr with
@@ -565,7 +702,7 @@ let emit_instr i =
           | Sixteen_unsigned
           | Sixteen_signed -> "strh"
           | Double
-          | Double_u -> "fstd"
+          | Double_u -> "vstr.64"
           | _ (* 32-bit quantities *) -> "str" in
         `	{emit_string instr}	{emit_reg r}, {emit_addressing addr i.arg 1}\n`; 1
     | Lop(Ialloc { words = n; label_after_call_gc; }) ->
@@ -623,9 +760,19 @@ let emit_instr i =
         `	smull	r12, {emit_reg i.res.(0)}, {emit_reg i.arg.(0)}, {emit_reg i.arg.(1)}\n`; 1
     | Lop(Ispecific Imulhadd) ->
         `	smmla	{emit_reg i.res.(0)}, {emit_reg i.arg.(0)}, {emit_reg i.arg.(1)}, {emit_reg i.arg.(2)}\n`; 1
+    | Lop(Iintop Imod) ->
+        (* This path is only chosen for ARMv7s *)
+        `       sdiv    {emit_reg i.res.(0)}, {emit_reg i.arg.(0)}, {emit_reg i
+.arg.(1)}\n`;
+        `       mls     {emit_reg i.res.(0)}, {emit_reg i.res.(0)}, {emit_reg i
+.arg.(1)}, {emit_reg i.arg.(0)}\n`;
+        2
     | Lop(Iintop op) ->
         let instr = name_for_int_operation op in
         `	{emit_string instr}	{emit_reg i.res.(0)}, {emit_reg i.arg.(0)}, {emit_reg i.arg.(1)}\n`; 1
+    | Lop(Iintop_imm((Ilsl|Ilsr|Iasr), 0)) ->
+        (* because lsls/lsrs/asrs do not support an imm arg #0 in thumb mode *)
+        `	mov	{emit_reg i.res.(0)}, {emit_reg i.arg.(0)}\n`; 1
     | Lop(Iintop_imm(op, n)) ->
         let instr = name_for_int_operation op in
         `	{emit_string instr}	{emit_reg i.res.(0)}, {emit_reg i.arg.(0)}, #{emit_int n}\n`; 1
@@ -638,9 +785,9 @@ let emit_instr i =
         `	{emit_string instr}	{emit_reg i.res.(1)}, {emit_reg i.arg.(1)}, #0x80000000\n`; 1
     | Lop(Iabsf | Inegf | Ispecific Isqrtf as op) ->
         let instr = (match op with
-                       Iabsf            -> "fabsd"
-                     | Inegf            -> "fnegd"
-                     | Ispecific Isqrtf -> "fsqrtd"
+                       Iabsf            -> "vabs.f64"
+                     | Inegf            -> "vneg.f64"
+                     | Ispecific Isqrtf -> "vsqrt.f64"
                      | _                -> assert false) in
         `	{emit_string instr}	{emit_reg i.res.(0)}, {emit_reg i.arg.(0)}\n`; 1
     | Lop(Ifloatofint) ->
@@ -651,21 +798,21 @@ let emit_instr i =
         `	fmrs	{emit_reg i.res.(0)}, s14\n`; 2
     | Lop(Iaddf | Isubf | Imulf | Idivf | Ispecific Inegmulf as op) ->
         let instr = (match op with
-                       Iaddf              -> "faddd"
-                     | Isubf              -> "fsubd"
-                     | Imulf              -> "fmuld"
-                     | Idivf              -> "fdivd"
-                     | Ispecific Inegmulf -> "fnmuld"
+                       Iaddf              -> "vadd.f64"
+                     | Isubf              -> "vsub.f64"
+                     | Imulf              -> "vmul.f64"
+                     | Idivf              -> "vdiv.f64"
+                     | Ispecific Inegmulf -> "vnmul.f64"
                      | _                  -> assert false) in
         `	{emit_string instr}	{emit_reg i.res.(0)}, {emit_reg i.arg.(0)}, {emit_reg i.arg.(1)}\n`;
         1
     | Lop(Ispecific(Imuladdf | Inegmuladdf | Imulsubf | Inegmulsubf as op)) ->
         assert (i.res.(0).loc = i.arg.(0).loc);
         let instr = (match op with
-                       Imuladdf    -> "fmacd"
-                     | Inegmuladdf -> "fnmacd"
-                     | Imulsubf    -> "fmscd"
-                     | Inegmulsubf -> "fnmscd"
+                       Imuladdf    -> "vmla.f64"
+                     | Inegmuladdf -> "vmls.f64"
+                     | Imulsubf    -> "vnmls.f64"
+                     | Inegmulsubf -> "vnmla.f64"
                      | _ -> assert false) in
         `	{emit_string instr}	{emit_reg i.res.(0)}, {emit_reg i.arg.(1)}, {emit_reg i.arg.(2)}\n`;
         1
@@ -771,7 +918,9 @@ let emit_instr i =
              so we need to generate appropriate trampolines for all labels
              that appear before this switch instruction (PR#5623) *)
           let tramtbl = Array.copy jumptbl in
+          let base = new_label() in
           `	tbh	[pc, {emit_reg i.arg.(0)}, lsl #1]\n`;
+          `{emit_label base}:\n`;
           for j = 0 to Array.length tramtbl - 1 do
             let rec label i =
               match i.desc with
@@ -779,7 +928,7 @@ let emit_instr i =
               | Llabel lbl when lbl = tramtbl.(j) -> lbl
               | _ -> label i.next in
             tramtbl.(j) <- label i.next;
-            `	.short	({emit_label tramtbl.(j)}-.)/2+{emit_int j}\n`
+            `	.short	({emit_label tramtbl.(j)}-{emit_label base})/2\n`
           done;
           let sz = ref (1 + (Array.length jumptbl + 1) / 2) in
           (* Generate the necessary trampolines *)
@@ -790,7 +939,7 @@ let emit_instr i =
             end
           done;
           !sz
-        end else if not !Clflags.pic_code then begin
+        end else if not !Clflags.pic_code && not !pie_code then begin
           `	ldr	pc, [pc, {emit_reg i.arg.(0)}, lsl #2]\n`;
           `	nop\n`;
           for j = 0 to Array.length jumptbl - 1 do
@@ -893,11 +1042,14 @@ let fundecl fundecl =
   `	.text\n`;
   `	.align	2\n`;
   `	.globl	{emit_symbol fundecl.fun_name}\n`;
-  if !arch > ARMv6 && !thumb then
-    `	.thumb\n`
-  else
+  if !arch > ARMv6 && !thumb then begin
+    `	.thumb\n`;
+    if macosx then
+      `	.thumb_func {emit_symbol fundecl.fun_name}\n`
+  end else
     `	.arm\n`;
-  `	.type	{emit_symbol fundecl.fun_name}, %function\n`;
+  if not macosx then
+    `	.type	{emit_symbol fundecl.fun_name}, %function\n`;
   `{emit_symbol fundecl.fun_name}:\n`;
   emit_debug_info fundecl.fun_dbg;
   cfi_startproc();
@@ -916,8 +1068,10 @@ let fundecl fundecl =
   List.iter emit_call_gc !call_gc_sites;
   List.iter emit_call_bound_error !bound_error_sites;
   cfi_endproc();
-  `	.type	{emit_symbol fundecl.fun_name}, %function\n`;
-  `	.size	{emit_symbol fundecl.fun_name}, .-{emit_symbol fundecl.fun_name}\n`
+  if not macosx then begin
+    `	.type	{emit_symbol fundecl.fun_name}, %function\n`;
+    `	.size	{emit_symbol fundecl.fun_name}, .-{emit_symbol fundecl.fun_name}\n`
+  end
 
 (* Emission of data *)
 
@@ -943,22 +1097,27 @@ let data l =
 
 let begin_assembly() =
   reset_debug_info();
+  delayed_symbol_literals := StrMap.empty;
   `	.file	\"\"\n`;  (* PR#7037 *)
   `	.syntax	unified\n`;
-  begin match !arch with
-  | ARMv4   -> `	.arch	armv4t\n`
-  | ARMv5   -> `	.arch	armv5t\n`
-  | ARMv5TE -> `	.arch	armv5te\n`
-  | ARMv6   -> `	.arch	armv6\n`
-  | ARMv6T2 -> `	.arch	armv6t2\n`
-  | ARMv7   -> `	.arch	armv7-a\n`
-  | ARMv8   -> `	.arch	armv8-a\n`
+  if not macosx then begin
+    match !arch with
+    | ARMv4   -> `	.arch	armv4t\n`
+    | ARMv5   -> `	.arch	armv5t\n`
+    | ARMv5TE -> `	.arch	armv5te\n`
+    | ARMv6   -> `	.arch	armv6\n`
+    | ARMv6T2 -> `	.arch	armv6t2\n`
+    | ARMv7   -> `	.arch	armv7-a\n`
+    | ARMv7s  -> `	.arch	armv7-a\n`;
+    | ARMv8   -> `	.arch	armv8-a\n`
+                 `	.arch_extension idiv\n`;
   end;
-  begin match !fpu with
-    Soft      -> `	.fpu	softvfp\n`
-  | VFPv2     -> `	.fpu	vfpv2\n`
-  | VFPv3_D16 -> `	.fpu	vfpv3-d16\n`
-  | VFPv3     -> `	.fpu	vfpv3\n`
+  if not macosx then begin
+    match !fpu with
+      Soft      -> `	.fpu	softvfp\n`
+    | VFPv2     -> `	.fpu	vfpv2\n`
+    | VFPv3_D16 -> `	.fpu	vfpv3-d16\n`
+    | VFPv3     -> `	.fpu	vfpv3\n`
   end;
   `trap_ptr	.req	r8\n`;
   `alloc_ptr	.req	r10\n`;
@@ -970,12 +1129,16 @@ let begin_assembly() =
   let lbl_begin = Compilenv.make_symbol (Some "code_begin") in
   `	.text\n`;
   `	.globl	{emit_symbol lbl_begin}\n`;
+  if macosx && !thumb then
+    `	.thumb_func     {emit_symbol lbl_begin}\n`;
   `{emit_symbol lbl_begin}:\n`
 
 let end_assembly () =
   let lbl_end = Compilenv.make_symbol (Some "code_end") in
   `	.text\n`;
   `	.globl	{emit_symbol lbl_end}\n`;
+  if macosx && !thumb then
+    `	.thumb_func     {emit_symbol lbl_end}\n`;
   `{emit_symbol lbl_end}:\n`;
   let lbl_end = Compilenv.make_symbol (Some "data_end") in
   `	.data\n`;
@@ -986,23 +1149,38 @@ let end_assembly () =
   let lbl = Compilenv.make_symbol (Some "frametable") in
   `	.globl	{emit_symbol lbl}\n`;
   `{emit_symbol lbl}:\n`;
+  let efa_label kind lbl =
+    if macosx && !thumb && kind = "%function" then
+      `	.thumb_func     {emit_label lbl}\n`;
+    if not macosx then
+      `	.type	{emit_label lbl}, {emit_string kind}\n`;
+    `	.word	{emit_label lbl}\n` in
+  let efa_label_rel_macosx =
+    let sylab1 = new_label() in
+    let sylab2 = ref 99 in
+    (fun lbl ofs ->
+       incr sylab2;
+       `Lofs_{emit_int sylab1}_{emit_int !sylab2} =	{emit_label lbl} - . + {emit_int32 ofs}\n`;
+       `	.word	Lofs_{emit_int sylab1}_{emit_int !sylab2}\n`
+    ) in
+  let efa_label_rel_std lbl ofs =
+    `	.word	{emit_label lbl} - . + {emit_int32 ofs}\n` in
   emit_frames
-    { efa_code_label = (fun lbl ->
-                       `	.type	{emit_label lbl}, %function\n`;
-                       `	.word	{emit_label lbl}\n`);
-      efa_data_label = (fun lbl ->
-                       `	.type	{emit_label lbl}, %object\n`;
-                       `	.word	{emit_label lbl}\n`);
+    { efa_code_label = efa_label "%function";
+      efa_data_label = efa_label "%object";
       efa_16 = (fun n -> `	.short	{emit_int n}\n`);
       efa_32 = (fun n -> `	.long	{emit_int32 n}\n`);
       efa_word = (fun n -> `	.word	{emit_int n}\n`);
       efa_align = (fun n -> `	.align	{emit_int(Misc.log2 n)}\n`);
-      efa_label_rel = (fun lbl ofs ->
-                           `	.word	{emit_label lbl} - . + {emit_int32 ofs}\n`);
+      efa_label_rel =
+        if macosx then efa_label_rel_macosx else efa_label_rel_std;
       efa_def_label = (fun lbl -> `{emit_label lbl}:\n`);
       efa_string = (fun s -> emit_string_directive "	.asciz	" s) };
-  `	.type	{emit_symbol lbl}, %object\n`;
-  `	.size	{emit_symbol lbl}, .-{emit_symbol lbl}\n`;
+  if not macosx then begin
+    `	.type	{emit_symbol lbl}, %object\n`;
+    `	.size	{emit_symbol lbl}, .-{emit_symbol lbl}\n`;
+  end;
+  emit_delayed_symbol_literals();
   begin match Config.system with
     "linux_eabihf" | "linux_eabi" | "netbsd" ->
       (* Mark stack as non-executable *)
diff --git a/asmcomp/arm/proc.ml b/asmcomp/arm/proc.ml
index 9e1bb648e..2385bf6a5 100644
--- a/asmcomp/arm/proc.ml
+++ b/asmcomp/arm/proc.ml
@@ -22,6 +22,9 @@ open Reg
 open Arch
 open Mach
 
+let macosx =
+  Config.system = "macosx"
+
 (* Instruction selection *)
 
 let word_addressed = false
@@ -111,30 +114,35 @@ let loc_spacetime_node_hole = Reg.dummy  (* Spacetime unsupported *)
 
 (* Calling conventions *)
 
-let calling_conventions first_int last_int first_float last_float make_stack
-      arg =
+let no_max_words = 9999
+
+let calling_conventions first_int last_int first_float last_float max_words
+      make_stack arg =
   let loc = Array.make (Array.length arg) [| Reg.dummy |] in
   let int = ref first_int in
   let float = ref first_float in
   let ofs = ref 0 in
+  let w = ref 0 in
   for i = 0 to Array.length arg - 1 do
     match arg.(i) with
     | [| arg |] ->
       begin match arg.typ with
       | Val | Int | Addr as ty ->
-          if !int <= last_int then begin
+          if !int <= last_int && !w < max_words then begin
             loc.(i) <- [| phys_reg !int |];
-            incr int
+            incr int;
+            incr w
           end else begin
             loc.(i) <- [| stack_slot (make_stack !ofs) ty |];
             ofs := !ofs + size_int
           end
       | Float ->
-          assert (abi = EABI_HF);
+          assert (abi = EABI_HF || abi = EABI_APPLE);
           assert (!fpu >= VFPv2);
-          if !float <= last_float then begin
+          if !float <= last_float && !w < max_words then begin
             loc.(i) <- [| phys_reg !float |];
-            incr float
+            incr float;
+            w := !w + 2
           end else begin
             ofs := Misc.align !ofs size_float;
             loc.(i) <- [| stack_slot (make_stack !ofs) Float |];
@@ -196,29 +204,45 @@ let ensure_single_regs res =
 
 let loc_arguments arg =
   let (loc, alignment) =
-    calling_conventions 0 7 100 115 outgoing (single_regs arg)
+    calling_conventions 0 7 100 115 no_max_words outgoing (single_regs arg)
   in
   ensure_single_regs loc, alignment
 let loc_parameters arg =
-  let (loc, _) = calling_conventions 0 7 100 115 incoming (single_regs arg) in
+  let (loc, _) = calling_conventions 0 7 100 115 no_max_words incoming (single_regs arg) in
   ensure_single_regs loc
 let loc_results res =
   let (loc, _) =
-    calling_conventions 0 7 100 115 not_supported (single_regs res)
+    calling_conventions 0 7 100 115 no_max_words not_supported (single_regs res)
   in
   ensure_single_regs loc
 
-(* C calling convention:
+(* C calling convention for GNU:
      first integer args in r0...r3
      first float args in d0...d7 (EABI+VFP)
      remaining args on stack.
-   Return values in r0...r1 or d0. *)
+   Return values in r0...r1 or d0.
+
+   C calling convention for iOS:
+     first integer args in r0...r3
+     first float args in pairs of regs r0/r1, r1/r2, or r2/r3
+     remaining args on stack.
+   Return values in r0 or r0/r1 for floats.
+
+   iOS: As we cannot represent register pairs properly, we pretend here that
+   we have two float registers d0..d1 for arguments and one float reg d0 for
+   the result. When emitting code for Iextcall we move the floats to the
+   registers where they should really be. The max_words constraint ensures
+   that we don't put too many parameters into registers.
+ *)
 
 let loc_external_arguments arg =
-  calling_conventions 0 3 100 107 outgoing arg
+  if macosx then
+    calling_conventions 0 3 100 101 4 outgoing arg
+  else
+    calling_conventions 0 3 100 107 no_max_words outgoing arg
 let loc_external_results res =
   let (loc, _) =
-    calling_conventions 0 1 100 100 not_supported (single_regs res)
+    calling_conventions 0 1 100 100 no_max_words not_supported (single_regs res)
   in
   ensure_single_regs loc
 
@@ -247,7 +271,7 @@ let destroyed_at_c_call =
                          108;109;110;111;112;113;114;115;
                          116;117;118;119;120;121;122;123;
                          124;125;126;127;128;129;130;131]
-                    | EABI_HF ->    (* r4-r7, d8-d15 preserved *)
+                    | EABI_HF | EABI_APPLE ->    (* r4-r7, d8-d15 preserved *)
                         [0;1;2;3;8;
                          100;101;102;103;104;105;106;107;
                          116;117;118;119;120;121;122;123;
diff --git a/asmcomp/arm/selection.ml b/asmcomp/arm/selection.ml
index 747e86a2a..e6d1bdc78 100644
--- a/asmcomp/arm/selection.ml
+++ b/asmcomp/arm/selection.ml
@@ -61,6 +61,10 @@ let pseudoregs_for_operation op arg res =
      is also a result of the mul / mla operation. *)
     Iintop Imul | Ispecific Imuladd when !arch < ARMv6 ->
       (arg, [| res.(0); arg.(0) |])
+  (* For the emulation of Imod the result reg must not be the same as
+     an operand reg *)
+  | Iintop Imod ->
+      (arg, [| res.(0); arg.(0); arg.(1) |])
   (* For smull rdlo,rdhi,rn,rm (pre-ARMv6) the registers rdlo, rdhi and rn
      must be different.  Also, rdlo (whose contents we discard) is always
      forced to be r12 in proc.ml, which means that neither rdhi and rn can
@@ -213,11 +217,15 @@ method! select_operation op args dbg =
   | (Cmulhi, args) ->
       (Iintop Imulh, args)
   (* Turn integer division/modulus into runtime ABI calls *)
-  | (Cdivi, args) ->
+  | (Cdivi, args) when (abi = EABI || abi = EABI_HF) && !arch < ARMv7s ->
       (self#iextcall("__aeabi_idiv", false), args)
-  | (Cmodi, args) ->
+  | (Cdivi, args) when abi = EABI_APPLE && !arch < ARMv7s ->
+      (self#iextcall("__divsi3", false), args)
+  | (Cmodi, args) when (abi = EABI || abi = EABI_HF) && !arch < ARMv7s ->
       (* See above for fix up of return register *)
       (self#iextcall("__aeabi_idivmod", false), args)
+  | (Cmodi, args) when abi = EABI_APPLE && !arch < ARMv7s ->
+      (self#iextcall("__modsi3", false), args)
   (* Recognize 16-bit bswap instruction (ARMv6T2 because we need movt) *)
   | (Cextcall("caml_bswap16_direct", _, _, _), args) when !arch >= ARMv6T2 ->
       (Ispecific(Ibswap 16), args)
diff --git a/asmcomp/arm64/emit.mlp b/asmcomp/arm64/emit.mlp
index de15b744a..55fcbca70 100644
--- a/asmcomp/arm64/emit.mlp
+++ b/asmcomp/arm64/emit.mlp
@@ -26,6 +26,10 @@ open Reg
 open Mach
 open Linearize
 open Emitaux
+open Printf
+
+let macosx =
+  Config.system = "macosx"
 
 (* Tradeoff between code size and code speed *)
 
@@ -41,14 +45,31 @@ let reg_x15 = phys_reg 15
 
 (* Output a label *)
 
+let label_prefix =
+  if macosx then "L" else ".L"
+
 let emit_label lbl =
-  emit_string ".L"; emit_int lbl
+  emit_string label_prefix; emit_int lbl
 
 (* Symbols *)
 
 let emit_symbol s =
+  if macosx then emit_string "_";
   Emitaux.emit_symbol '$' s
 
+(* Object types *)
+
+let emit_type emit_lbl_or_sym lbl_or_sym ty =
+  if not macosx then begin
+    `	.type	{emit_lbl_or_sym lbl_or_sym}, %{emit_string ty}\n`
+  end
+
+
+let emit_symbol_size sym =
+  if not macosx then begin
+    `	.size	{emit_symbol sym}, .-{emit_symbol sym}\n`
+  end
+
 (* Output a pseudo-register *)
 
 let emit_reg = function
@@ -313,6 +334,8 @@ let float_literal f =
 (* Emit all pending literals *)
 let emit_literals() =
   if !float_literals <> [] then begin
+    if macosx then
+      `	.section	__TEXT,__literal8,8byte_literals\n`;
     `	.align	3\n`;
     List.iter
       (fun (f, lbl) ->
@@ -324,7 +347,11 @@ let emit_literals() =
 (* Emit code to load the address of a symbol *)
 
 let emit_load_symbol_addr dst s =
-  if not !Clflags.dlcode then begin
+  if macosx then begin
+    `	adrp	{emit_reg dst}, {emit_symbol s}@GOTPAGE\n`;
+    `	ldr	{emit_reg dst}, [{emit_reg dst}, {emit_symbol s}@GOTPAGEOFF]\n`
+  end
+  else if (not !Clflags.dlcode) || Compilenv.symbol_in_current_unit s then begin
     `	adrp	{emit_reg dst}, {emit_symbol s}\n`;
     `	add	{emit_reg dst}, {emit_reg dst}, #:lo12:{emit_symbol s}\n`
   end else begin
@@ -554,6 +581,18 @@ let assembly_code_for_allocation ?label_after_call_gc i ~n ~far =
     `{emit_label lbl_frame}:	add	{emit_reg i.res.(0)}, {emit_reg reg_alloc_ptr}, #8\n`
   end
 
+(* Emit code to load an emitted literal *)
+
+let emit_load_literal dst lbl =
+  if macosx then begin
+    `	adrp	{emit_reg reg_tmp1}, {emit_label lbl}@PAGE\n`;
+    `	ldr	{emit_reg dst}, [{emit_reg reg_tmp1}, {emit_label lbl}@PAGEOFF]\n`
+  end else begin
+    `	adrp	{emit_reg reg_tmp1}, {emit_label lbl}\n`;
+    `	ldr	{emit_reg dst}, [{emit_reg reg_tmp1}, #:lo12:{emit_label lbl}]\n`
+  end
+
+
 (* Output the assembly code for an instruction *)
 
 let emit_instr i =
@@ -581,11 +620,13 @@ let emit_instr i =
         if f = 0L then
           `	fmov	{emit_reg i.res.(0)}, xzr\n`
         else if is_immediate_float f then
-          `	fmov	{emit_reg i.res.(0)}, #{emit_printf "0x%Lx" f}\n`
+          let s =
+             if macosx then sprintf "%1.6e" (Int64.float_of_bits f)
+             else sprintf "0x%Lx" f in
+          `	fmov	{emit_reg i.res.(0)}, #{emit_string s}\n`
         else begin
           let lbl = float_literal f in
-          `	adrp	{emit_reg reg_tmp1}, {emit_label lbl}\n`;
-          `	ldr	{emit_reg i.res.(0)}, [{emit_reg reg_tmp1}, #:lo12:{emit_label lbl}]\n`
+          emit_load_literal i.res.(0) lbl
         end
     | Lop(Iconst_symbol s) ->
         emit_load_symbol_addr i.res.(0) s
@@ -854,12 +895,12 @@ let emit_instr i =
         `{emit_label lblnext}:\n`
     | Lpushtrap ->
         stack_offset := !stack_offset + 16;
-        `	str	{emit_reg reg_trap_ptr}, [sp, -16]!\n`;
+        `	str	{emit_reg reg_trap_ptr}, [sp, #-16]!\n`;
         `	str	{emit_reg reg_tmp1}, [sp, #8]\n`;
         cfi_adjust_cfa_offset 16;
         `	mov	{emit_reg reg_trap_ptr}, sp\n`
     | Lpoptrap ->
-        `	ldr	{emit_reg reg_trap_ptr}, [sp], 16\n`;
+        `	ldr	{emit_reg reg_trap_ptr}, [sp], #16\n`;
         cfi_adjust_cfa_offset (-16);
         stack_offset := !stack_offset - 16
     | Lraise k ->
@@ -870,7 +911,7 @@ let emit_instr i =
         | Cmm.Raise_notrace ->
           `	mov	sp, {emit_reg reg_trap_ptr}\n`;
           `	ldr	{emit_reg reg_tmp1}, [sp, #8]\n`;
-          `	ldr	{emit_reg reg_trap_ptr}, [sp], 16\n`;
+          `	ldr	{emit_reg reg_trap_ptr}, [sp], #16\n`;
           `	br	{emit_reg reg_tmp1}\n`
         end
 
@@ -903,7 +944,7 @@ let fundecl fundecl =
   `	.text\n`;
   `	.align	3\n`;
   `	.globl	{emit_symbol fundecl.fun_name}\n`;
-  `	.type	{emit_symbol fundecl.fun_name}, %function\n`;
+  emit_type emit_symbol fundecl.fun_name "function";
   `{emit_symbol fundecl.fun_name}:\n`;
   emit_debug_info fundecl.fun_dbg;
   cfi_startproc();
@@ -930,8 +971,8 @@ let fundecl fundecl =
   assert (List.length !call_gc_sites = num_call_gc);
   assert (List.length !bound_error_sites = num_check_bound);
   cfi_endproc();
-  `	.type	{emit_symbol fundecl.fun_name}, %function\n`;
-  `	.size	{emit_symbol fundecl.fun_name}, .-{emit_symbol fundecl.fun_name}\n`;
+  emit_type emit_symbol fundecl.fun_name "function";
+  emit_symbol_size fundecl.fun_name;
   emit_literals()
 
 (* Emission of data *)
@@ -988,16 +1029,17 @@ let end_assembly () =
   `	.globl	{emit_symbol lbl_end}\n`;
   `{emit_symbol lbl_end}:\n`;
   `	.long	0\n`;
+  `	.long	0\n`;
   let lbl = Compilenv.make_symbol (Some "frametable") in
   `	.globl	{emit_symbol lbl}\n`;
   `{emit_symbol lbl}:\n`;
   emit_frames
     { efa_code_label = (fun lbl ->
-                       `	.type	{emit_label lbl}, %function\n`;
-                       `	.quad	{emit_label lbl}\n`);
+                        emit_type emit_label lbl "function";
+                        `  .quad {emit_label lbl}\n`);
       efa_data_label = (fun lbl ->
-                       `	.type	{emit_label lbl}, %object\n`;
-                       `	.quad	{emit_label lbl}\n`);
+                        emit_type emit_label lbl "object";
+                        `  .quad {emit_label lbl}\n`);
       efa_16 = (fun n -> `	.short	{emit_int n}\n`);
       efa_32 = (fun n -> `	.long	{emit_int32 n}\n`);
       efa_word = (fun n -> `	.quad	{emit_int n}\n`);
@@ -1006,8 +1048,8 @@ let end_assembly () =
                            `	.long	{emit_label lbl} - . + {emit_int32 ofs}\n`);
       efa_def_label = (fun lbl -> `{emit_label lbl}:\n`);
       efa_string = (fun s -> emit_string_directive "	.asciz	" s) };
-  `	.type	{emit_symbol lbl}, %object\n`;
-  `	.size	{emit_symbol lbl}, .-{emit_symbol lbl}\n`;
+  emit_type emit_symbol lbl "object";
+  emit_symbol_size lbl;
   begin match Config.system with
   | "linux" ->
       (* Mark stack as non-executable *)
diff --git a/asmcomp/arm64/selection.ml b/asmcomp/arm64/selection.ml
index b714d0032..5c1825efc 100644
--- a/asmcomp/arm64/selection.ml
+++ b/asmcomp/arm64/selection.ml
@@ -21,6 +21,9 @@ open Arch
 open Cmm
 open Mach
 
+let macosx =
+  Config.system = "macosx"
+
 let is_offset chunk n =
    (n >= -256 && n <= 255)               (* 9 bits signed unscaled *)
 || (n >= 0 &&
@@ -83,7 +86,8 @@ let inline_ops =
     "caml_int64_direct_bswap"; "caml_nativeint_direct_bswap" ]
 
 let use_direct_addressing _symb =
-  not !Clflags.dlcode
+  (not !Clflags.dlcode) &&
+    not macosx
 
 (* Instruction selection *)
 
@@ -92,9 +96,7 @@ class selector = object(self)
 inherit Selectgen.selector_generic as super
 
 method is_immediate n =
-  let mn = -n in
   n land 0xFFF = n || n land 0xFFF_000 = n
-  || mn land 0xFFF = mn || mn land 0xFFF_000 = mn
 
 method! is_simple_expr = function
   (* inlined floating-point ops are simple if their arguments are *)
@@ -131,11 +133,13 @@ method! select_operation op args dbg =
       begin match args with
       (* Add immediate *)
       | [arg; Cconst_int n] when self#is_immediate n ->
-          ((if n >= 0 then Iintop_imm(Iadd, n) else Iintop_imm(Isub, -n)),
-           [arg])
+          (Iintop_imm(Iadd, n), [arg])
       | [Cconst_int n; arg] when self#is_immediate n ->
-          ((if n >= 0 then Iintop_imm(Iadd, n) else Iintop_imm(Isub, -n)),
-           [arg])
+          (Iintop_imm(Iadd, n), [arg])
+      | [arg; Cconst_int n] when self#is_immediate(-n) ->
+          (Iintop_imm(Isub, -n), [arg])
+      | [Cconst_int n; arg] when self#is_immediate(-n) ->
+          (Iintop_imm(Isub, -n), [arg])
       (* Shift-add *)
       | [arg1; Cop(Clsl, [arg2; Cconst_int n], _)] when n > 0 && n < 64 ->
           (Ispecific(Ishiftarith(Ishiftadd, n)), [arg1; arg2])
@@ -163,8 +167,9 @@ method! select_operation op args dbg =
       begin match args with
       (* Sub immediate *)
       | [arg; Cconst_int n] when self#is_immediate n ->
-          ((if n >= 0 then Iintop_imm(Isub, n) else Iintop_imm(Iadd, -n)),
-           [arg])
+          (Iintop_imm(Isub, n), [arg])
+      | [arg; Cconst_int n] when self#is_immediate n ->
+          (Iintop_imm(Iadd, -n), [arg])
       (* Shift-sub *)
       | [arg1; Cop(Clsl, [arg2; Cconst_int n], _)] when n > 0 && n < 64 ->
           (Ispecific(Ishiftarith(Ishiftsub, n)), [arg1; arg2])
diff --git a/asmrun/arm.S b/asmrun/arm.S
index 12bc4a1b9..20472ca04 100644
--- a/asmrun/arm.S
+++ b/asmrun/arm.S
@@ -21,6 +21,70 @@
 
         .syntax unified
         .text
+#if defined(SYS_macosx)
+#define Glo(s) _##s
+#define Loc(s) L##s
+#define P(s) LP##s
+#if defined(MODEL_armv6)
+        .arm
+        .macro  .funtype
+        .endm
+        .macro  cbz
+        cmp     $0, #0
+        beq     $1
+        .endm
+        /* load global address, non-PIE */
+        .macro ldgaddr
+        ldr $0, P($1)
+        .endm
+        /* indirection for global address, non-PIE */
+        .macro gaddr
+P($0):  .word Glo($0)
+        .endm
+        /* load local address, non-PIE */
+        .macro ldladdr
+        ldr $0, P($1)
+        .endm
+        /* indirection for local address, non-PIE */
+        .macro laddr
+P($0):  .word Loc($0)
+        .endm
+#else
+        /* for ARM>=7 we can easily support PIE because we have movw and movt */
+        .thumb
+        Lpcreloffs = 4     /* for .arm: 8 */
+        .macro .funtype
+        .thumb_func $0
+        .endm
+        /* load global address, PIE */
+        .macro ldgaddr
+        movw $0, #:lower16:(P($1) - (9f + Lpcreloffs))
+        movt $0, #:upper16:(P($1) - (9f + Lpcreloffs))
+9:
+        add $0, pc               /* actually $0 := $0 + pc + Lpcreloffs */
+        ldr $0, [$0]
+        .endm
+        /* indirection for global address, PIE */
+        .macro gaddr
+P($0):  .indirect_symbol Glo($0)
+        .long 0
+        .endm
+        /* load local address, PIE */
+        .macro ldladdr
+        movw $0, #:lower16:(Loc($1) - (9f + Lpcreloffs))
+        movt $0, #:upper16:(Loc($1) - (9f + Lpcreloffs))
+9:
+        add $0, pc               /* actually $0 := $0 + pc + Lpcreloffs */
+        .endm
+        /* indirection for local address, PIE */
+        .macro laddr
+        .endm
+#endif
+        .macro  .type
+        .endm
+        .macro  .size
+        .endm
+#endif
 #if defined(SYS_linux_eabihf) && defined(MODEL_armv6)
         .arch   armv6
         .fpu    vfpv2
@@ -78,6 +142,26 @@
         beq     \lbl
         .endm
 #endif
+#if !defined(SYS_macosx)
+#define Glo(s) s
+#define Loc(s) .L##s
+#define P(s) .LP##s
+        .macro  .funtype symbol
+        .type  \symbol, %function
+        .endm
+        /* load global address, non-PIE GNU */
+        .macro  ldgaddr dest, src
+        ldr \dest, =Glo(\src)
+        .endm
+        .macro gaddr ref
+        .endm
+        /* load local address, non-PIE GNU */
+        .macro  ldladdr dest, src
+        ldr \dest, =Loc(\src)
+        .endm
+        .macro laddr ref
+        .endm
+#endif
 
 trap_ptr        .req    r8
 alloc_ptr       .req    r10
@@ -113,22 +197,22 @@ alloc_limit     .req    r11
 
 /* Allocation functions and GC interface */
 
-        .globl  caml_system__code_begin
-caml_system__code_begin:
+        .globl  Glo(caml_system__code_begin)
+Glo(caml_system__code_begin):
 
         .align  2
-        .globl  caml_call_gc
-caml_call_gc:
+        .globl  Glo(caml_call_gc)
+Glo(caml_call_gc):
         CFI_STARTPROC
         PROFILE
     /* Record return address */
-        ldr     r12, =caml_last_return_address
+        ldgaddr r12, caml_last_return_address
         str     lr, [r12]
-.Lcaml_call_gc:
+Loc(caml_call_gc):
     /* Record lowest stack address */
-        ldr     r12, =caml_bottom_of_stack
+        ldgaddr r12, caml_bottom_of_stack
         str     sp, [r12]
-#if defined(SYS_linux_eabihf) || defined(SYS_netbsd)
+#if defined(SYS_linux_eabihf) || defined(SYS_netbsd) || defined(SYS_macosx)
     /* Save caller floating-point registers on the stack */
         vpush   {d0-d7}; CFI_ADJUST(64)
 #endif
@@ -140,175 +224,174 @@ caml_call_gc:
         CFI_OFFSET(lr, -4)
 #endif
     /* Store pointer to saved integer registers in caml_gc_regs */
-        ldr     r12, =caml_gc_regs
+        ldgaddr r12, caml_gc_regs
         str     sp, [r12]
     /* Save current allocation pointer for debugging purposes */
-        ldr     alloc_limit, =caml_young_ptr
+        ldgaddr alloc_limit, caml_young_ptr
         str     alloc_ptr, [alloc_limit]
     /* Save trap pointer in case an exception is raised during GC */
-        ldr     r12, =caml_exception_pointer
+        ldgaddr r12, caml_exception_pointer
         str     trap_ptr, [r12]
     /* Call the garbage collector */
-        bl      caml_garbage_collection
+        bl      Glo(caml_garbage_collection)
     /* Restore integer registers and return address from the stack */
         pop     {r0-r7,r12,lr}; CFI_ADJUST(-40)
-#if defined(SYS_linux_eabihf) || defined(SYS_netbsd)
+#if defined(SYS_linux_eabihf) || defined(SYS_netbsd) || defined(SYS_macosx)
     /* Restore floating-point registers from the stack */
         vpop    {d0-d7}; CFI_ADJUST(-64)
 #endif
     /* Reload new allocation pointer and limit */
     /* alloc_limit still points to caml_young_ptr */
-        ldr     r12, =caml_young_limit
+        ldgaddr r12, caml_young_limit
         ldr     alloc_ptr, [alloc_limit]
         ldr     alloc_limit, [r12]
     /* Return to caller */
         bx      lr
         CFI_ENDPROC
-        .type   caml_call_gc, %function
-        .size   caml_call_gc, .-caml_call_gc
+        .funtype Glo(caml_call_gc)
+        .size    Glo(caml_call_gc), .-Glo(caml_call_gc)
 
         .align  2
-        .globl  caml_alloc1
-caml_alloc1:
+        .globl  Glo(caml_alloc1)
+Glo(caml_alloc1):
         CFI_STARTPROC
         PROFILE
-.Lcaml_alloc1:
+Loc(caml_alloc1):
         sub     alloc_ptr, alloc_ptr, 8
         cmp     alloc_ptr, alloc_limit
         bcc     1f
         bx      lr
 1:  /* Record return address */
-        ldr     r7, =caml_last_return_address
+        ldgaddr r7, caml_last_return_address
         str     lr, [r7]
     /* Call GC (preserves r7) */
-        bl      .Lcaml_call_gc
+        bl      Loc(caml_call_gc)
     /* Restore return address */
         ldr     lr, [r7]
     /* Try again */
-        b       .Lcaml_alloc1
+        b       Loc(caml_alloc1)
         CFI_ENDPROC
-        .type   caml_alloc1, %function
-        .size   caml_alloc1, .-caml_alloc1
+        .funtype Glo(caml_alloc1)
+        .size    Glo(caml_alloc1), .-Glo(caml_alloc1)
 
         .align  2
-        .globl  caml_alloc2
-caml_alloc2:
+        .globl  Glo(caml_alloc2)
+Glo(caml_alloc2):
         CFI_STARTPROC
         PROFILE
-.Lcaml_alloc2:
+Loc(caml_alloc2):
         sub     alloc_ptr, alloc_ptr, 12
         cmp     alloc_ptr, alloc_limit
         bcc     1f
         bx      lr
 1:  /* Record return address */
-        ldr     r7, =caml_last_return_address
+        ldgaddr r7, caml_last_return_address
         str     lr, [r7]
     /* Call GC (preserves r7) */
-        bl      .Lcaml_call_gc
+        bl      Loc(caml_call_gc)
     /* Restore return address */
         ldr     lr, [r7]
     /* Try again */
-        b       .Lcaml_alloc2
+        b       Loc(caml_alloc2)
         CFI_ENDPROC
-        .type   caml_alloc2, %function
-        .size   caml_alloc2, .-caml_alloc2
+        .funtype Glo(caml_alloc2)
+        .size    Glo(caml_alloc2), .-Glo(caml_alloc2)
 
         .align  2
-        .globl  caml_alloc3
-        .type caml_alloc3, %function
-caml_alloc3:
+        .globl  Glo(caml_alloc3)
+Glo(caml_alloc3):
         CFI_STARTPROC
         PROFILE
-.Lcaml_alloc3:
+Loc(caml_alloc3):
         sub     alloc_ptr, alloc_ptr, 16
         cmp     alloc_ptr, alloc_limit
         bcc     1f
         bx      lr
 1:  /* Record return address */
-        ldr     r7, =caml_last_return_address
+        ldgaddr r7, caml_last_return_address
         str     lr, [r7]
     /* Call GC (preserves r7) */
-        bl      .Lcaml_call_gc
+        bl      Loc(caml_call_gc)
     /* Restore return address */
         ldr     lr, [r7]
     /* Try again */
-        b       .Lcaml_alloc3
+        b       Loc(caml_alloc3)
         CFI_ENDPROC
-        .type   caml_alloc3, %function
-        .size   caml_alloc3, .-caml_alloc3
+        .funtype Glo(caml_alloc3)
+        .size    Glo(caml_alloc3), .-Glo(caml_alloc3)
 
         .align  2
-        .globl  caml_allocN
-caml_allocN:
+        .globl  Glo(caml_allocN)
+Glo(caml_allocN):
         CFI_STARTPROC
         PROFILE
-.Lcaml_allocN:
+Loc(caml_allocN):
         sub     alloc_ptr, alloc_ptr, r7
         cmp     alloc_ptr, alloc_limit
         bcc     1f
         bx      lr
 1:  /* Record return address */
-        ldr     r12, =caml_last_return_address
+        ldgaddr r12, caml_last_return_address
         str     lr, [r12]
     /* Call GC (preserves r7) */
-        bl      .Lcaml_call_gc
+        bl      Loc(caml_call_gc)
     /* Restore return address */
-        ldr     r12, =caml_last_return_address
+        ldgaddr r12, caml_last_return_address
         ldr     lr, [r12]
     /* Try again */
-        b       .Lcaml_allocN
+        b       Loc(caml_allocN)
         CFI_ENDPROC
-        .type   caml_allocN, %function
-        .size   caml_allocN, .-caml_allocN
+        .funtype Glo(caml_allocN)
+        .size    Glo(caml_allocN), .-Glo(caml_allocN)
 
 /* Call a C function from OCaml */
 /* Function to call is in r7 */
 
         .align  2
-        .globl  caml_c_call
-caml_c_call:
+        .globl  Glo(caml_c_call)
+Glo(caml_c_call):
         CFI_STARTPROC
         PROFILE
     /* Record lowest stack address and return address */
-        ldr     r5, =caml_last_return_address
-        ldr     r6, =caml_bottom_of_stack
+        ldgaddr r5, caml_last_return_address
+        ldgaddr r6, caml_bottom_of_stack
         str     lr, [r5]
         str     sp, [r6]
     /* Preserve return address in callee-save register r4 */
         mov     r4, lr
         CFI_REGISTER(lr, r4)
     /* Make the exception handler alloc ptr available to the C code */
-        ldr     r5, =caml_young_ptr
-        ldr     r6, =caml_exception_pointer
+        ldgaddr r5, caml_young_ptr
+        ldgaddr r6, caml_exception_pointer
         str     alloc_ptr, [r5]
         str     trap_ptr, [r6]
     /* Call the function */
         blx     r7
     /* Reload alloc ptr and alloc limit */
-        ldr     r6, =caml_young_limit
+        ldgaddr r6, caml_young_limit
         ldr     alloc_ptr, [r5]         /* r5 still points to caml_young_ptr */
         ldr     alloc_limit, [r6]
     /* Return */
         bx      r4
         CFI_ENDPROC
-        .type   caml_c_call, %function
-        .size   caml_c_call, .-caml_c_call
+        .funtype Glo(caml_c_call)
+        .size    Glo(caml_c_call), .-Glo(caml_c_call)
 
 /* Start the OCaml program */
 
         .align  2
-        .globl  caml_start_program
-caml_start_program:
+        .globl  Glo(caml_start_program)
+Glo(caml_start_program):
         CFI_STARTPROC
         PROFILE
-        ldr     r12, =caml_program
+        ldgaddr r12, caml_program
 
 /* Code shared with caml_callback* */
 /* Address of OCaml code to call is in r12 */
 /* Arguments to the OCaml code are in r0...r3 */
 
-.Ljump_to_caml:
-#if defined(SYS_linux_eabihf) || defined(SYS_netbsd)
+Loc(jump_to_caml):
+#if defined(SYS_linux_eabihf) || defined(SYS_netbsd) || defined(SYS_macosx)
     /* Save callee-save floating-point registers */
         vpush   {d8-d15}; CFI_ADJUST(64)
 #endif
@@ -321,9 +404,9 @@ caml_start_program:
 #endif
     /* Setup a callback link on the stack */
         sub     sp, sp, 16; CFI_ADJUST(16)              /* 8-byte alignment */
-        ldr     r4, =caml_bottom_of_stack
-        ldr     r5, =caml_last_return_address
-        ldr     r6, =caml_gc_regs
+        ldgaddr r4, caml_bottom_of_stack
+        ldgaddr r5, caml_last_return_address
+        ldgaddr r6, caml_gc_regs
         ldr     r4, [r4]
         ldr     r5, [r5]
         ldr     r6, [r6]
@@ -332,78 +415,78 @@ caml_start_program:
         str     r6, [sp, 8]
     /* Setup a trap frame to catch exceptions escaping the OCaml code */
         sub     sp, sp, 8; CFI_ADJUST(8)
-        ldr     r6, =caml_exception_pointer
-        ldr     r5, =.Ltrap_handler
+        ldgaddr r6, caml_exception_pointer
+        ldladdr r5, trap_handler
         ldr     r4, [r6]
         str     r4, [sp, 0]
         str     r5, [sp, 4]
         mov     trap_ptr, sp
     /* Reload allocation pointers */
-        ldr     r4, =caml_young_ptr
+        ldgaddr r4, caml_young_ptr
         ldr     alloc_ptr, [r4]
-        ldr     r4, =caml_young_limit
+        ldgaddr r4, caml_young_limit
         ldr     alloc_limit, [r4]
     /* Call the OCaml code */
         blx     r12
-.Lcaml_retaddr:
+Loc(caml_retaddr):
     /* Pop the trap frame, restoring caml_exception_pointer */
-        ldr     r4, =caml_exception_pointer
+        ldgaddr r4, caml_exception_pointer
         ldr     r5, [sp, 0]
         str     r5, [r4]
         add     sp, sp, 8; CFI_ADJUST(-8)
     /* Pop the callback link, restoring the global variables */
-.Lreturn_result:
-        ldr     r4, =caml_bottom_of_stack
+Loc(return_result):
+        ldgaddr r4, caml_bottom_of_stack
         ldr     r5, [sp, 0]
         str     r5, [r4]
-        ldr     r4, =caml_last_return_address
+        ldgaddr r4, caml_last_return_address
         ldr     r5, [sp, 4]
         str     r5, [r4]
-        ldr     r4, =caml_gc_regs
+        ldgaddr r4, caml_gc_regs
         ldr     r5, [sp, 8]
         str     r5, [r4]
         add     sp, sp, 16; CFI_ADJUST(-16)
     /* Update allocation pointer */
-        ldr     r4, =caml_young_ptr
+        ldgaddr r4, caml_young_ptr
         str     alloc_ptr, [r4]
     /* Reload callee-save registers and return address */
         pop     {r4-r8,r10,r11,lr}; CFI_ADJUST(-32)
-#if defined(SYS_linux_eabihf) || defined(SYS_netbsd)
+#if defined(SYS_linux_eabihf) || defined(SYS_netbsd) || defined(SYS_macosx)
     /* Reload callee-save floating-point registers */
         vpop    {d8-d15}; CFI_ADJUST(-64)
 #endif
         bx      lr
         CFI_ENDPROC
-        .type   .Lcaml_retaddr, %function
-        .size   .Lcaml_retaddr, .-.Lcaml_retaddr
-        .type   caml_start_program, %function
-        .size   caml_start_program, .-caml_start_program
+        .funtype Loc(caml_retaddr)
+        .size    Loc(caml_retaddr), .-Loc(caml_retaddr)
+        .funtype Glo(caml_start_program)
+        .size    Glo(caml_start_program), .-Glo(caml_start_program)
 
 /* The trap handler */
 
         .align  2
-.Ltrap_handler:
+Loc(trap_handler):
         CFI_STARTPROC
     /* Save exception pointer */
-        ldr     r12, =caml_exception_pointer
+        ldgaddr r12, caml_exception_pointer
         str     trap_ptr, [r12]
     /* Encode exception bucket as an exception result */
         orr     r0, r0, 2
     /* Return it */
-        b       .Lreturn_result
+        b       Loc(return_result)
         CFI_ENDPROC
-        .type   .Ltrap_handler, %function
-        .size   .Ltrap_handler, .-.Ltrap_handler
+        .funtype Loc(trap_handler)
+        .size    Loc(trap_handler), .-Loc(trap_handler)
 
 /* Raise an exception from OCaml */
 
         .align  2
-        .globl  caml_raise_exn
-caml_raise_exn:
+        .globl  Glo(caml_raise_exn)
+Glo(caml_raise_exn):
         CFI_STARTPROC
         PROFILE
     /* Test if backtrace is active */
-        ldr     r1, =caml_backtrace_active
+        ldgaddr r1, caml_backtrace_active
         ldr     r1, [r1]
         cbz     r1, 1f
     /* Preserve exception bucket in callee-save register r4 */
@@ -412,7 +495,7 @@ caml_raise_exn:
         mov     r1, lr                          /* arg2: pc of raise */
         mov     r2, sp                          /* arg3: sp of raise */
         mov     r3, trap_ptr                    /* arg4: sp of handler */
-        bl      caml_stash_backtrace
+        bl      Glo(caml_stash_backtrace)
     /* Restore exception bucket */
         mov     r0, r4
 1:  /* Cut stack at current trap handler */
@@ -420,35 +503,35 @@ caml_raise_exn:
     /* Pop previous handler and addr of trap, and jump to it */
         pop     {trap_ptr, pc}
         CFI_ENDPROC
-        .type   caml_raise_exn, %function
-        .size   caml_raise_exn, .-caml_raise_exn
+        .funtype Glo(caml_raise_exn)
+        .size    Glo(caml_raise_exn), .-Glo(caml_raise_exn)
 
 /* Raise an exception from C */
 
         .align  2
-        .globl  caml_raise_exception
-caml_raise_exception:
+        .globl  Glo(caml_raise_exception)
+Glo(caml_raise_exception):
         CFI_STARTPROC
         PROFILE
     /* Reload trap ptr, alloc ptr and alloc limit */
-        ldr     trap_ptr, =caml_exception_pointer
-        ldr     alloc_ptr, =caml_young_ptr
-        ldr     alloc_limit, =caml_young_limit
+        ldgaddr trap_ptr, caml_exception_pointer
+        ldgaddr alloc_ptr, caml_young_ptr
+        ldgaddr alloc_limit, caml_young_limit
         ldr     trap_ptr, [trap_ptr]
         ldr     alloc_ptr, [alloc_ptr]
         ldr     alloc_limit, [alloc_limit]
     /* Test if backtrace is active */
-        ldr     r1, =caml_backtrace_active
+        ldgaddr r1, caml_backtrace_active
         ldr     r1, [r1]
         cbz     r1, 1f
     /* Preserve exception bucket in callee-save register r4 */
         mov     r4, r0
-        ldr     r1, =caml_last_return_address   /* arg2: pc of raise */
+        ldgaddr r1, caml_last_return_address   /* arg2: pc of raise */
         ldr     r1, [r1]
-        ldr     r2, =caml_bottom_of_stack       /* arg3: sp of raise */
+        ldgaddr r2, caml_bottom_of_stack       /* arg3: sp of raise */
         ldr     r2, [r2]
         mov     r3, trap_ptr                    /* arg4: sp of handler */
-        bl      caml_stash_backtrace
+        bl      Glo(caml_stash_backtrace)
     /* Restore exception bucket */
         mov     r0, r4
 1:  /* Cut stack at current trap handler */
@@ -456,14 +539,14 @@ caml_raise_exception:
     /* Pop previous handler and addr of trap, and jump to it */
         pop     {trap_ptr, pc}
         CFI_ENDPROC
-        .type   caml_raise_exception, %function
-        .size   caml_raise_exception, .-caml_raise_exception
+        .funtype Glo(caml_raise_exception)
+        .size    Glo(caml_raise_exception), .-Glo(caml_raise_exception)
 
 /* Callback from C to OCaml */
 
         .align  2
-        .globl  caml_callback_exn
-caml_callback_exn:
+        .globl  Glo(caml_callback_exn)
+Glo(caml_callback_exn):
         CFI_STARTPROC
         PROFILE
     /* Initial shuffling of arguments (r0 = closure, r1 = first arg) */
@@ -471,14 +554,14 @@ caml_callback_exn:
         mov     r0, r1          /* r0 = first arg */
         mov     r1, r12         /* r1 = closure environment */
         ldr     r12, [r12]      /* code pointer */
-        b       .Ljump_to_caml
+        b       Loc(jump_to_caml)
         CFI_ENDPROC
-        .type   caml_callback_exn, %function
-        .size   caml_callback_exn, .-caml_callback_exn
+        .funtype Glo(caml_callback_exn)
+        .size    Glo(caml_callback_exn), .-Glo(caml_callback_exn)
 
         .align  2
-        .globl  caml_callback2_exn
-caml_callback2_exn:
+        .globl  Glo(caml_callback2_exn)
+Glo(caml_callback2_exn):
         CFI_STARTPROC
         PROFILE
     /* Initial shuffling of arguments (r0 = closure, r1 = arg1, r2 = arg2) */
@@ -486,15 +569,15 @@ caml_callback2_exn:
         mov     r0, r1          /* r0 = first arg */
         mov     r1, r2          /* r1 = second arg */
         mov     r2, r12         /* r2 = closure environment */
-        ldr     r12, =caml_apply2
-        b       .Ljump_to_caml
+        ldgaddr r12, caml_apply2
+        b       Loc(jump_to_caml)
         CFI_ENDPROC
-        .type   caml_callback2_exn, %function
-        .size   caml_callback2_exn, .-caml_callback2_exn
+        .funtype Glo(caml_callback2_exn)
+        .size    Glo(caml_callback2_exn), .-Glo(caml_callback2_exn)
 
         .align  2
-        .globl  caml_callback3_exn
-caml_callback3_exn:
+        .globl  Glo(caml_callback3_exn)
+Glo(caml_callback3_exn):
         CFI_STARTPROC
         PROFILE
     /* Initial shuffling of arguments */
@@ -504,41 +587,63 @@ caml_callback3_exn:
         mov     r1, r2          /* r1 = second arg */
         mov     r2, r3          /* r2 = third arg */
         mov     r3, r12         /* r3 = closure environment */
-        ldr     r12, =caml_apply3
-        b       .Ljump_to_caml
+        ldgaddr r12, caml_apply3
+        b       Loc(jump_to_caml)
         CFI_ENDPROC
-        .type   caml_callback3_exn, %function
-        .size   caml_callback3_exn, .-caml_callback3_exn
+        .funtype Glo(caml_callback3_exn)
+        .size    Glo(caml_callback3_exn), .-Glo(caml_callback3_exn)
 
         .align  2
-        .globl  caml_ml_array_bound_error
-caml_ml_array_bound_error:
+        .globl  Glo(caml_ml_array_bound_error)
+Glo(caml_ml_array_bound_error):
         CFI_STARTPROC
         PROFILE
     /* Load address of [caml_array_bound_error] in r7 */
-        ldr     r7, =caml_array_bound_error
+        ldgaddr r7, caml_array_bound_error
     /* Call that function */
-        b       caml_c_call
+        b       Glo(caml_c_call)
         CFI_ENDPROC
-        .type   caml_ml_array_bound_error, %function
-        .size   caml_ml_array_bound_error, .-caml_ml_array_bound_error
+        .funtype Glo(caml_ml_array_bound_error)
+        .size    Glo(caml_ml_array_bound_error), .-Glo(caml_ml_array_bound_error)
 
-        .globl  caml_system__code_end
-caml_system__code_end:
+/* Indirection block for macosx */
+#if defined(SYS_macosx)
+#if !defined(MODEL_armv6)
+        .non_lazy_symbol_pointer
+#endif
+        .align 2
+         gaddr caml_last_return_address
+         gaddr caml_bottom_of_stack
+         gaddr caml_gc_regs
+         gaddr caml_young_ptr
+         gaddr caml_exception_pointer
+         gaddr caml_young_limit
+         gaddr caml_program
+         gaddr caml_backtrace_active
+         gaddr caml_apply2
+         gaddr caml_apply3
+         gaddr caml_array_bound_error
+         laddr trap_handler
+#endif
+
+        .globl  Glo(caml_system__code_end)
+Glo(caml_system__code_end):
 
 /* GC roots for callback */
 
         .data
         .align  2
-        .globl  caml_system__frametable
-caml_system__frametable:
+        .globl  Glo(caml_system__frametable)
+Glo(caml_system__frametable):
         .word   1               /* one descriptor */
-        .word   .Lcaml_retaddr  /* return address into callback */
+        .word   Loc(caml_retaddr)  /* return address into callback */
         .short  -1              /* negative frame size => use callback link */
         .short  0               /* no roots */
         .align  2
+#ifndef SYS_macosx
         .type   caml_system__frametable, %object
         .size   caml_system__frametable, .-caml_system__frametable
 
 /* Mark stack as non-executable */
         .section .note.GNU-stack,"",%progbits
+#endif
diff --git a/asmrun/arm64.S b/asmrun/arm64.S
index dea5ab731..beab6680b 100644
--- a/asmrun/arm64.S
+++ b/asmrun/arm64.S
@@ -18,6 +18,33 @@
 
 #include "caml/m.h"
 
+/* Globals and labels */
+#if defined(SYS_macosx)
+#define G(sym) _##sym
+#define L(lbl) L##lbl
+#else
+#define G(sym) sym
+#define L(lbl) .L##lbl
+#endif
+
+/* Functions */
+
+#if defined(SYS_macosx)
+        .macro FUNCTION name
+        .endm
+        .macro OBJECT name
+        .endm
+#else
+        .macro FUNCTION name
+        .type  \name, %function
+        .size  \name, .-\name
+        .endm
+        .macro OBJECT name
+        .type  \name, %object
+        .size  \name, .-\name
+        .endm
+#endif
+
 /* Special registers */
 
 #define TRAP_PTR x26
@@ -49,141 +76,157 @@
 
 /* Macros to load and store global variables.  Destroy TMP2 */
 
-#if defined(__PIC__)
-
-#define ADDRGLOBAL(reg,symb) \
-        adrp    TMP2, :got:symb; \
-        ldr     reg, [TMP2, #:got_lo12:symb]
-
-#define LOADGLOBAL(reg,symb) \
-        ADDRGLOBAL(TMP2,symb); \
-        ldr     reg, [TMP2]
-
-#define STOREGLOBAL(reg,symb) \
-        ADDRGLOBAL(TMP2,symb); \
-        str     reg, [TMP2]
-
+#if defined(SYS_macosx)
+        .macro ADDRGLOBAL reg, symb
+        adrp    TMP2, \symb@GOTPAGE
+        ldr     \reg, [TMP2, \symb@GOTPAGEOFF]
+        .endm
+
+        .macro LOADGLOBAL reg, symb
+        ADDRGLOBAL TMP2, \symb
+        ldr     \reg, [TMP2]
+        .endm
+
+        .macro STOREGLOBAL reg, symb
+        ADDRGLOBAL TMP2, \symb
+        str     \reg, [TMP2]
+        .endm
+#elif defined(__PIC__)
+        .macro ADDRGLOBAL reg, symb
+        adrp    TMP2, :got:\symb
+        ldr     \reg, [TMP2, #:got_lo12:\symb]
+        .endm
+
+        .macro LOADGLOBAL reg, symb
+        ADDRGLOBAL TMP2, \symb
+        ldr     \reg, [TMP2]
+        .endm
+
+        .macro STOREGLOBAL reg, symb
+        ADDRGLOBAL TMP2, \symb
+        str     \reg, [TMP2]
+        .endm
 #else
-
-#define ADDRGLOBAL(reg,symb) \
-        adrp    reg, symb; \
-        add     reg, reg, #:lo12:symb
-
-#define LOADGLOBAL(reg,symb) \
-        adrp    TMP2, symb; \
-        ldr     reg, [TMP2, #:lo12:symb]
-
-#define STOREGLOBAL(reg,symb) \
-        adrp    TMP2, symb; \
-        str     reg, [TMP2, #:lo12:symb]
-
+        .macro ADDRGLOBAL reg, symb
+        adrp    \reg, \symb
+        ldr     \reg, \reg, #:lo12:symb
+        .endm
+
+        .macro LOADGLOBAL reg, symb
+        adrp    TMP2, \symb
+        ldr     \reg, [TMP2, #:lo12:\symb]
+        .endm
+
+        .macro STOREGLOBAL reg, symb
+        adrp    TMP2, \symb
+        str     \reg, [TMP2, #:lo12:\symb]
+        .endm
 #endif
 
 /* Allocation functions and GC interface */
 
-        .globl  caml_system__code_begin
-caml_system__code_begin:
+        .globl  G(caml_system__code_begin)
+G(caml_system__code_begin):
 
         .align  2
-        .globl  caml_call_gc
-caml_call_gc:
+        .globl  G(caml_call_gc)
+G(caml_call_gc):
         CFI_STARTPROC
         PROFILE
     /* Record return address */
-        STOREGLOBAL(x30, caml_last_return_address)
+        STOREGLOBAL x30, G(caml_last_return_address)
     /* Record lowest stack address */
         mov     TMP, sp
-        STOREGLOBAL(TMP, caml_bottom_of_stack)
-.Lcaml_call_gc:
+        STOREGLOBAL TMP, G(caml_bottom_of_stack)
+L(caml_call_gc):
     /* Set up stack space, saving return address and frame pointer */
     /* (2 regs RA/GP, 24 allocatable int regs, 24 caller-save float regs) * 8 */
         CFI_OFFSET(29, -400)
         CFI_OFFSET(30, -392)
-        stp     x29, x30, [sp, -400]!
+        stp     x29, x30, [sp, #-400]!
         CFI_ADJUST(400)
         add     x29, sp, #0
     /* Save allocatable integer registers on the stack, in the order
        given in proc.ml */
-        stp     x0, x1, [sp, 16]
-        stp     x2, x3, [sp, 32]
-        stp     x4, x5, [sp, 48]
-        stp     x6, x7, [sp, 64]
-        stp     x8, x9, [sp, 80]
-        stp     x10, x11, [sp, 96]
-        stp     x12, x13, [sp, 112]
-        stp     x14, x15, [sp, 128]
-        stp     x19, x20, [sp, 144]
-        stp     x21, x22, [sp, 160]
-        stp     x23, x24, [sp, 176]
-        str     x25, [sp, 192]
+        stp     x0, x1, [sp, #16]
+        stp     x2, x3, [sp, #32]
+        stp     x4, x5, [sp, #48]
+        stp     x6, x7, [sp, #64]
+        stp     x8, x9, [sp, #80]
+        stp     x10, x11, [sp, #96]
+        stp     x12, x13, [sp, #112]
+        stp     x14, x15, [sp, #128]
+        stp     x19, x20, [sp, #144]
+        stp     x21, x22, [sp, #160]
+        stp     x23, x24, [sp, #176]
+        str     x25, [sp, #192]
      /* Save caller-save floating-point registers on the stack
         (callee-saves are preserved by caml_garbage_collection) */
-        stp     d0, d1, [sp, 208]
-        stp     d2, d3, [sp, 224]
-        stp     d4, d5, [sp, 240]
-        stp     d6, d7, [sp, 256]
-        stp     d16, d17, [sp, 272]
-        stp     d18, d19, [sp, 288]
-        stp     d20, d21, [sp, 304]
-        stp     d22, d23, [sp, 320]
-        stp     d24, d25, [sp, 336]
-        stp     d26, d27, [sp, 352]
-        stp     d28, d29, [sp, 368]
-        stp     d30, d31, [sp, 384]
+        stp     d0, d1, [sp, #208]
+        stp     d2, d3, [sp, #224]
+        stp     d4, d5, [sp, #240]
+        stp     d6, d7, [sp, #256]
+        stp     d16, d17, [sp, #272]
+        stp     d18, d19, [sp, #288]
+        stp     d20, d21, [sp, #304]
+        stp     d22, d23, [sp, #320]
+        stp     d24, d25, [sp, #336]
+        stp     d26, d27, [sp, #352]
+        stp     d28, d29, [sp, #368]
+        stp     d30, d31, [sp, #384]
     /* Store pointer to saved integer registers in caml_gc_regs */
         add     TMP, sp, #16
-        STOREGLOBAL(TMP, caml_gc_regs)
+        STOREGLOBAL TMP, G(caml_gc_regs)
     /* Save current allocation pointer for debugging purposes */
-        STOREGLOBAL(ALLOC_PTR, caml_young_ptr)
+        STOREGLOBAL ALLOC_PTR, G(caml_young_ptr)
     /* Save trap pointer in case an exception is raised during GC */
-        STOREGLOBAL(TRAP_PTR, caml_exception_pointer)
+        STOREGLOBAL TRAP_PTR, G(caml_exception_pointer)
     /* Call the garbage collector */
-        bl      caml_garbage_collection
+        bl      G(caml_garbage_collection)
     /* Restore registers */
-        ldp     x0, x1, [sp, 16]
-        ldp     x2, x3, [sp, 32]
-        ldp     x4, x5, [sp, 48]
-        ldp     x6, x7, [sp, 64]
-        ldp     x8, x9, [sp, 80]
-        ldp     x10, x11, [sp, 96]
-        ldp     x12, x13, [sp, 112]
-        ldp     x14, x15, [sp, 128]
-        ldp     x19, x20, [sp, 144]
-        ldp     x21, x22, [sp, 160]
-        ldp     x23, x24, [sp, 176]
-        ldr     x25, [sp, 192]
-        ldp     d0, d1, [sp, 208]
-        ldp     d2, d3, [sp, 224]
-        ldp     d4, d5, [sp, 240]
-        ldp     d6, d7, [sp, 256]
-        ldp     d16, d17, [sp, 272]
-        ldp     d18, d19, [sp, 288]
-        ldp     d20, d21, [sp, 304]
-        ldp     d22, d23, [sp, 320]
-        ldp     d24, d25, [sp, 336]
-        ldp     d26, d27, [sp, 352]
-        ldp     d28, d29, [sp, 368]
-        ldp     d30, d31, [sp, 384]
+        ldp     x0, x1, [sp, #16]
+        ldp     x2, x3, [sp, #32]
+        ldp     x4, x5, [sp, #48]
+        ldp     x6, x7, [sp, #64]
+        ldp     x8, x9, [sp, #80]
+        ldp     x10, x11, [sp, #96]
+        ldp     x12, x13, [sp, #112]
+        ldp     x14, x15, [sp, #128]
+        ldp     x19, x20, [sp, #144]
+        ldp     x21, x22, [sp, #160]
+        ldp     x23, x24, [sp, #176]
+        ldr     x25, [sp, #192]
+        ldp     d0, d1, [sp, #208]
+        ldp     d2, d3, [sp, #224]
+        ldp     d4, d5, [sp, #240]
+        ldp     d6, d7, [sp, #256]
+        ldp     d16, d17, [sp, #272]
+        ldp     d18, d19, [sp, #288]
+        ldp     d20, d21, [sp, #304]
+        ldp     d22, d23, [sp, #320]
+        ldp     d24, d25, [sp, #336]
+        ldp     d26, d27, [sp, #352]
+        ldp     d28, d29, [sp, #368]
+        ldp     d30, d31, [sp, #384]
     /* Reload new allocation pointer and allocation limit */
-        LOADGLOBAL(ALLOC_PTR, caml_young_ptr)
-        LOADGLOBAL(ALLOC_LIMIT, caml_young_limit)
+        LOADGLOBAL ALLOC_PTR, G(caml_young_ptr)
+        LOADGLOBAL ALLOC_LIMIT, G(caml_young_limit)
     /* Free stack space and return to caller */
-        ldp     x29, x30, [sp], 400
+        ldp     x29, x30, [sp], #400
         ret
         CFI_ENDPROC
-        .type   caml_call_gc, %function
-        .size   caml_call_gc, .-caml_call_gc
+        FUNCTION G(caml_call_gc)
 
         .align  2
-        .globl  caml_alloc1
-caml_alloc1:
+        .globl  G(caml_alloc1)
+G(caml_alloc1):
         CFI_STARTPROC
         PROFILE
 1:      sub     ALLOC_PTR, ALLOC_PTR, #16
         cmp     ALLOC_PTR, ALLOC_LIMIT
         b.lo    2f
         ret
-2:      stp     x29, x30, [sp, -16]!
+2:      stp     x29, x30, [sp, #-16]!
         CFI_ADJUST(16)
     /* Record the lowest address of the caller's stack frame.  This is the
        address immediately above the pair of words (x29 and x30) we just
@@ -192,249 +235,241 @@ caml_alloc1:
        frame won't match the frame size contained in the relevant frame
        descriptor. */
         add     x29, sp, #16
-        STOREGLOBAL(x29, caml_bottom_of_stack)
+        STOREGLOBAL x29, G(caml_bottom_of_stack)
         add     x29, sp, #0
     /* Record return address */
-        STOREGLOBAL(x30, caml_last_return_address)
+        STOREGLOBAL x30, G(caml_last_return_address)
     /* Call GC */
-        bl      .Lcaml_call_gc
+        bl      L(caml_call_gc)
     /* Restore return address */
-        ldp     x29, x30, [sp], 16
+        ldp     x29, x30, [sp], #16
         CFI_ADJUST(-16)
     /* Try again */
         b       1b
         CFI_ENDPROC
-        .type   caml_alloc1, %function
-        .size   caml_alloc1, .-caml_alloc1
+        FUNCTION G(caml_alloc1)
 
         .align  2
-        .globl  caml_alloc2
-caml_alloc2:
+        .globl  G(caml_alloc2)
+G(caml_alloc2):
         CFI_STARTPROC
         PROFILE
 1:      sub     ALLOC_PTR, ALLOC_PTR, #24
         cmp     ALLOC_PTR, ALLOC_LIMIT
         b.lo    2f
         ret
-2:      stp     x29, x30, [sp, -16]!
+2:      stp     x29, x30, [sp, #-16]!
         CFI_ADJUST(16)
     /* Record the lowest address of the caller's stack frame.
        See comment above. */
         add     x29, sp, #16
-        STOREGLOBAL(x29, caml_bottom_of_stack)
+        STOREGLOBAL x29, G(caml_bottom_of_stack)
         add     x29, sp, #0
     /* Record return address */
-        STOREGLOBAL(x30, caml_last_return_address)
+        STOREGLOBAL x30, G(caml_last_return_address)
     /* Call GC */
-        bl      .Lcaml_call_gc
+        bl      L(caml_call_gc)
     /* Restore return address */
-        ldp     x29, x30, [sp], 16
+        ldp     x29, x30, [sp], #16
         CFI_ADJUST(-16)
     /* Try again */
         b       1b
         CFI_ENDPROC
-        .type   caml_alloc2, %function
-        .size   caml_alloc2, .-caml_alloc2
+        FUNCTION G(caml_alloc2)
 
         .align  2
-        .globl  caml_alloc3
-caml_alloc3:
+        .globl  G(caml_alloc3)
+G(caml_alloc3):
         CFI_STARTPROC
         PROFILE
 1:      sub     ALLOC_PTR, ALLOC_PTR, #32
         cmp     ALLOC_PTR, ALLOC_LIMIT
         b.lo    2f
         ret
-2:      stp     x29, x30, [sp, -16]!
+2:      stp     x29, x30, [sp, #-16]!
         CFI_ADJUST(16)
     /* Record the lowest address of the caller's stack frame.
        See comment above. */
         add     x29, sp, #16
-        STOREGLOBAL(x29, caml_bottom_of_stack)
+        STOREGLOBAL x29, G(caml_bottom_of_stack)
         add     x29, sp, #0
     /* Record return address */
-        STOREGLOBAL(x30, caml_last_return_address)
+        STOREGLOBAL x30, G(caml_last_return_address)
     /* Call GC */
-        bl      .Lcaml_call_gc
+        bl      L(caml_call_gc)
     /* Restore return address */
-        ldp     x29, x30, [sp], 16
+        ldp     x29, x30, [sp], #16
         CFI_ADJUST(-16)
     /* Try again */
         b       1b
         CFI_ENDPROC
-        .type   caml_alloc3, %function
-        .size   caml_alloc3, .-caml_alloc3
+        FUNCTION G(caml_alloc3)
 
         .align  2
-        .globl  caml_allocN
-caml_allocN:
+        .globl  G(caml_allocN)
+G(caml_allocN):
         CFI_STARTPROC
         PROFILE
 1:      sub     ALLOC_PTR, ALLOC_PTR, ARG
         cmp     ALLOC_PTR, ALLOC_LIMIT
         b.lo    2f
         ret
-2:      stp     x29, x30, [sp, -16]!
+2:      stp     x29, x30, [sp, #-16]!
         CFI_ADJUST(16)
     /* Record the lowest address of the caller's stack frame.
        See comment above. */
         add     x29, sp, #16
-        STOREGLOBAL(x29, caml_bottom_of_stack)
+        STOREGLOBAL x29, G(caml_bottom_of_stack)
         add     x29, sp, #0
     /* Record return address */
-        STOREGLOBAL(x30, caml_last_return_address)
+        STOREGLOBAL x30, G(caml_last_return_address)
     /* Call GC.  This preserves ARG */
-        bl      .Lcaml_call_gc
+        bl      L(caml_call_gc)
     /* Restore return address */
-        ldp     x29, x30, [sp], 16
+        ldp     x29, x30, [sp], #16
         CFI_ADJUST(-16)
     /* Try again */
         b       1b
         CFI_ENDPROC
-        .type   caml_allocN, %function
-        .size   caml_allocN, .-caml_allocN
+        FUNCTION G(caml_allocN)
 
 /* Call a C function from OCaml */
 /* Function to call is in ARG */
 
         .align  2
-        .globl  caml_c_call
-caml_c_call:
+        .globl  G(caml_c_call)
+G(caml_c_call):
         CFI_STARTPROC
         PROFILE
     /* Preserve return address in callee-save register x19 */
         mov     x19, x30
         CFI_REGISTER(30, 19)
     /* Record lowest stack address and return address */
-        STOREGLOBAL(x30, caml_last_return_address)
+        STOREGLOBAL x30, G(caml_last_return_address)
         add     TMP, sp, #0
-        STOREGLOBAL(TMP, caml_bottom_of_stack)
+        STOREGLOBAL TMP, G(caml_bottom_of_stack)
     /* Make the exception handler alloc ptr available to the C code */
-        STOREGLOBAL(ALLOC_PTR, caml_young_ptr)
-        STOREGLOBAL(TRAP_PTR, caml_exception_pointer)
+        STOREGLOBAL ALLOC_PTR, G(caml_young_ptr)
+        STOREGLOBAL TRAP_PTR, G(caml_exception_pointer)
     /* Call the function */
         blr     ARG
     /* Reload alloc ptr and alloc limit */
-        LOADGLOBAL(ALLOC_PTR, caml_young_ptr)
-        LOADGLOBAL(ALLOC_LIMIT, caml_young_limit)
+        LOADGLOBAL ALLOC_PTR, G(caml_young_ptr)
+        LOADGLOBAL ALLOC_LIMIT, G(caml_young_limit)
     /* Return */
         ret     x19
         CFI_ENDPROC
-        .type   caml_c_call, %function
-        .size   caml_c_call, .-caml_c_call
+        FUNCTION G(caml_c_call)
 
 /* Start the OCaml program */
 
         .align  2
-        .globl  caml_start_program
-caml_start_program:
+        .globl  G(caml_start_program)
+G(caml_start_program):
         CFI_STARTPROC
         PROFILE
-        ADDRGLOBAL(ARG, caml_program)
+        ADDRGLOBAL ARG, G(caml_program)
 
 /* Code shared with caml_callback* */
 /* Address of OCaml code to call is in ARG */
 /* Arguments to the OCaml code are in x0...x7 */
 
-.Ljump_to_caml:
+L(jump_to_caml):
     /* Set up stack frame and save callee-save registers */
         CFI_OFFSET(29, -160)
         CFI_OFFSET(30, -152)
-        stp     x29, x30, [sp, -160]!
+        stp     x29, x30, [sp, #-160]!
         CFI_ADJUST(160)
         add     x29, sp, #0
-        stp     x19, x20, [sp, 16]
-        stp     x21, x22, [sp, 32]
-        stp     x23, x24, [sp, 48]
-        stp     x25, x26, [sp, 64]
-        stp     x27, x28, [sp, 80]
-        stp     d8, d9, [sp, 96]
-        stp     d10, d11, [sp, 112]
-        stp     d12, d13, [sp, 128]
-        stp     d14, d15, [sp, 144]
+        stp     x19, x20, [sp, #16]
+        stp     x21, x22, [sp, #32]
+        stp     x23, x24, [sp, #48]
+        stp     x25, x26, [sp, #64]
+        stp     x27, x28, [sp, #80]
+        stp     d8, d9, [sp, #96]
+        stp     d10, d11, [sp, #112]
+        stp     d12, d13, [sp, #128]
+        stp     d14, d15, [sp, #144]
     /* Setup a callback link on the stack */
-        LOADGLOBAL(x8, caml_bottom_of_stack)
-        LOADGLOBAL(x9, caml_last_return_address)
-        LOADGLOBAL(x10, caml_gc_regs)
-        stp     x8, x9, [sp, -32]!     /* 16-byte alignment */
+        LOADGLOBAL x8, G(caml_bottom_of_stack)
+        LOADGLOBAL x9, G(caml_last_return_address)
+        LOADGLOBAL x10, G(caml_gc_regs)
+        stp     x8, x9, [sp, #-32]!     /* 16-byte alignment */
         CFI_ADJUST(32)
-        str     x10, [sp, 16]
+        str     x10, [sp, #16]
     /* Setup a trap frame to catch exceptions escaping the OCaml code */
-        LOADGLOBAL(x8, caml_exception_pointer)
-        adr     x9, .Ltrap_handler
-        stp     x8, x9, [sp, -16]!
+        LOADGLOBAL x8, G(caml_exception_pointer)
+        adr     x9, L(trap_handler)
+        stp     x8, x9, [sp, #-16]!
         CFI_ADJUST(16)
         add     TRAP_PTR, sp, #0
     /* Reload allocation pointers */
-        LOADGLOBAL(ALLOC_PTR, caml_young_ptr)
-        LOADGLOBAL(ALLOC_LIMIT, caml_young_limit)
+        LOADGLOBAL ALLOC_PTR, G(caml_young_ptr)
+        LOADGLOBAL ALLOC_LIMIT, G(caml_young_limit)
     /* Call the OCaml code */
         blr     ARG
-.Lcaml_retaddr:
+L(caml_retaddr):
     /* Pop the trap frame, restoring caml_exception_pointer */
-        ldr     x8, [sp], 16
+        ldr     x8, [sp], #16
         CFI_ADJUST(-16)
-        STOREGLOBAL(x8, caml_exception_pointer)
+        STOREGLOBAL x8, G(caml_exception_pointer)
     /* Pop the callback link, restoring the global variables */
-.Lreturn_result:
-        ldr     x10, [sp, 16]
-        ldp     x8, x9, [sp], 32
+L(return_result):
+        ldr     x10, [sp, #16]
+        ldp     x8, x9, [sp], #32
         CFI_ADJUST(-32)
-        STOREGLOBAL(x8, caml_bottom_of_stack)
-        STOREGLOBAL(x9, caml_last_return_address)
-        STOREGLOBAL(x10, caml_gc_regs)
+        STOREGLOBAL x8, G(caml_bottom_of_stack)
+        STOREGLOBAL x9, G(caml_last_return_address)
+        STOREGLOBAL x10, G(caml_gc_regs)
     /* Update allocation pointer */
-        STOREGLOBAL(ALLOC_PTR, caml_young_ptr)
+        STOREGLOBAL ALLOC_PTR, G(caml_young_ptr)
     /* Reload callee-save registers and return address */
-        ldp     x19, x20, [sp, 16]
-        ldp     x21, x22, [sp, 32]
-        ldp     x23, x24, [sp, 48]
-        ldp     x25, x26, [sp, 64]
-        ldp     x27, x28, [sp, 80]
-        ldp     d8, d9, [sp, 96]
-        ldp     d10, d11, [sp, 112]
-        ldp     d12, d13, [sp, 128]
-        ldp     d14, d15, [sp, 144]
-        ldp     x29, x30, [sp], 160
+        ldp     x19, x20, [sp, #16]
+        ldp     x21, x22, [sp, #32]
+        ldp     x23, x24, [sp, #48]
+        ldp     x25, x26, [sp, #64]
+        ldp     x27, x28, [sp, #80]
+        ldp     d8, d9, [sp, #96]
+        ldp     d10, d11, [sp, #112]
+        ldp     d12, d13, [sp, #128]
+        ldp     d14, d15, [sp, #144]
+        ldp     x29, x30, [sp], #160
         CFI_ADJUST(-160)
     /* Return to C caller */
         ret
         CFI_ENDPROC
-        .type   .Lcaml_retaddr, %function
-        .size   .Lcaml_retaddr, .-.Lcaml_retaddr
-        .type   caml_start_program, %function
-        .size   caml_start_program, .-caml_start_program
+        FUNCTION L(caml_retaddr)
+        FUNCTION G(caml_start_program)
 
 /* The trap handler */
 
         .align  2
-.Ltrap_handler:
+L(trap_handler):
         CFI_STARTPROC
     /* Save exception pointer */
-        STOREGLOBAL(TRAP_PTR, caml_exception_pointer)
+        STOREGLOBAL TRAP_PTR, G(caml_exception_pointer)
     /* Encode exception bucket as an exception result */
         orr     x0, x0, #2
     /* Return it */
-        b       .Lreturn_result
+        b       L(return_result)
         CFI_ENDPROC
-        .type   .Ltrap_handler, %function
-        .size   .Ltrap_handler, .-.Ltrap_handler
+        FUNCTION L(trap_handler)
 
 /* Raise an exception from OCaml */
 
         .align  2
-        .globl  caml_raise_exn
-caml_raise_exn:
+        .globl  G(caml_raise_exn)
+G(caml_raise_exn):
         CFI_STARTPROC
         PROFILE
     /* Test if backtrace is active */
-        LOADGLOBAL(TMP, caml_backtrace_active)
+        LOADGLOBAL TMP, G(caml_backtrace_active)
         cbnz     TMP, 2f
 1:  /* Cut stack at current trap handler */
         mov     sp, TRAP_PTR
     /* Pop previous handler and jump to it */
-        ldr     TMP, [sp, 8]
-        ldr     TRAP_PTR, [sp], 16
+        ldr     TMP, [sp, #8]
+        ldr     TRAP_PTR, [sp], #16
         br      TMP
 2:  /* Preserve exception bucket in callee-save register x19 */
         mov     x19, x0
@@ -443,54 +478,52 @@ caml_raise_exn:
         mov     x1, x30        /* arg2: pc of raise */
         add     x2, sp, #0     /* arg3: sp of raise */
         mov     x3, TRAP_PTR   /* arg4: sp of handler */
-        bl      caml_stash_backtrace
+        bl      G(caml_stash_backtrace)
     /* Restore exception bucket and raise */
         mov     x0, x19
         b       1b
         CFI_ENDPROC
-        .type   caml_raise_exn, %function
-        .size   caml_raise_exn, .-caml_raise_exn
+        FUNCTION G(caml_raise_exn)
 
 /* Raise an exception from C */
 
         .align  2
-        .globl  caml_raise_exception
-caml_raise_exception:
+        .globl  G(caml_raise_exception)
+G(caml_raise_exception):
         CFI_STARTPROC
         PROFILE
     /* Reload trap ptr, alloc ptr and alloc limit */
-        LOADGLOBAL(TRAP_PTR, caml_exception_pointer)
-        LOADGLOBAL(ALLOC_PTR, caml_young_ptr)
-        LOADGLOBAL(ALLOC_LIMIT, caml_young_limit)
+        LOADGLOBAL TRAP_PTR, G(caml_exception_pointer)
+        LOADGLOBAL ALLOC_PTR, G(caml_young_ptr)
+        LOADGLOBAL ALLOC_LIMIT, G(caml_young_limit)
     /* Test if backtrace is active */
-        LOADGLOBAL(TMP, caml_backtrace_active)
+        LOADGLOBAL TMP, G(caml_backtrace_active)
         cbnz    TMP, 2f
 1:  /* Cut stack at current trap handler */
         mov     sp, TRAP_PTR
     /* Pop previous handler and jump to it */
-        ldr     TMP, [sp, 8]
-        ldr     TRAP_PTR, [sp], 16
+        ldr     TMP, [sp, #8]
+        ldr     TRAP_PTR, [sp], #16
         br      TMP
 2:  /* Preserve exception bucket in callee-save register x19 */
         mov     x19, x0
     /* Stash the backtrace */
                                /* arg1: exn bucket, already in x0 */
-        LOADGLOBAL(x1, caml_last_return_address)   /* arg2: pc of raise */
-        LOADGLOBAL(x2, caml_bottom_of_stack)       /* arg3: sp of raise */
+        LOADGLOBAL x1, G(caml_last_return_address)   /* arg2: pc of raise */
+        LOADGLOBAL x2, G(caml_bottom_of_stack)       /* arg3: sp of raise */
         mov     x3, TRAP_PTR   /* arg4: sp of handler */
-        bl      caml_stash_backtrace
+        bl      G(caml_stash_backtrace)
     /* Restore exception bucket and raise */
         mov     x0, x19
         b       1b
         CFI_ENDPROC
-        .type   caml_raise_exception, %function
-        .size   caml_raise_exception, .-caml_raise_exception
+        FUNCTION G(caml_raise_exception)
 
 /* Callback from C to OCaml */
 
         .align  2
-        .globl  caml_callback_exn
-caml_callback_exn:
+        .globl  G(caml_callback_exn)
+G(caml_callback_exn):
         CFI_STARTPROC
         PROFILE
     /* Initial shuffling of arguments (x0 = closure, x1 = first arg) */
@@ -498,14 +531,13 @@ caml_callback_exn:
         mov     x0, x1          /* x0 = first arg */
         mov     x1, TMP         /* x1 = closure environment */
         ldr     ARG, [TMP]      /* code pointer */
-        b       .Ljump_to_caml
+        b       L(jump_to_caml)
         CFI_ENDPROC
-        .type   caml_callback_exn, %function
-        .size   caml_callback_exn, .-caml_callback_exn
+        FUNCTION G(caml_callback_exn)
 
         .align  2
-        .globl  caml_callback2_exn
-caml_callback2_exn:
+        .globl  G(caml_callback2_exn)
+G(caml_callback2_exn):
         CFI_STARTPROC
         PROFILE
     /* Initial shuffling of arguments (x0 = closure, x1 = arg1, x2 = arg2) */
@@ -513,15 +545,14 @@ caml_callback2_exn:
         mov     x0, x1          /* x0 = first arg */
         mov     x1, x2          /* x1 = second arg */
         mov     x2, TMP         /* x2 = closure environment */
-        ADDRGLOBAL(ARG, caml_apply2)
-        b       .Ljump_to_caml
+        ADDRGLOBAL ARG, G(caml_apply2)
+        b       L(jump_to_caml)
         CFI_ENDPROC
-        .type   caml_callback2_exn, %function
-        .size   caml_callback2_exn, .-caml_callback2_exn
+        FUNCTION G(caml_callback2_exn)
 
         .align  2
-        .globl  caml_callback3_exn
-caml_callback3_exn:
+        .globl  G(caml_callback3_exn)
+G(caml_callback3_exn):
         CFI_STARTPROC
         PROFILE
     /* Initial shuffling of arguments */
@@ -531,41 +562,40 @@ caml_callback3_exn:
         mov     x1, x2          /* x1 = second arg */
         mov     x2, x3          /* x2 = third arg */
         mov     x3, TMP         /* x3 = closure environment */
-        ADDRGLOBAL(ARG, caml_apply3)
-        b       .Ljump_to_caml
+        ADDRGLOBAL ARG, G(caml_apply3)
+        b       L(jump_to_caml)
         CFI_ENDPROC
-        .type   caml_callback3_exn, %function
-        .size   caml_callback3_exn, .-caml_callback3_exn
+        FUNCTION G(caml_callback3_exn)
 
         .align  2
-        .globl  caml_ml_array_bound_error
-caml_ml_array_bound_error:
+        .globl  G(caml_ml_array_bound_error)
+G(caml_ml_array_bound_error):
         CFI_STARTPROC
         PROFILE
     /* Load address of [caml_array_bound_error] in ARG */
-        ADDRGLOBAL(ARG, caml_array_bound_error)
+        ADDRGLOBAL ARG, G(caml_array_bound_error)
     /* Call that function */
-        b       caml_c_call
+        b       G(caml_c_call)
         CFI_ENDPROC
-        .type   caml_ml_array_bound_error, %function
-        .size   caml_ml_array_bound_error, .-caml_ml_array_bound_error
+        FUNCTION G(caml_ml_array_bound_error)
 
-        .globl  caml_system__code_end
-caml_system__code_end:
+        .globl  G(caml_system__code_end)
+G(caml_system__code_end):
 
 /* GC roots for callback */
 
         .data
         .align  3
-        .globl  caml_system__frametable
-caml_system__frametable:
+        .globl  G(caml_system__frametable)
+G(caml_system__frametable):
         .quad   1               /* one descriptor */
-        .quad   .Lcaml_retaddr  /* return address into callback */
+        .quad   L(caml_retaddr) /* return address into callback */
         .short  -1              /* negative frame size => use callback link */
         .short  0               /* no roots */
         .align  3
-        .type   caml_system__frametable, %object
-        .size   caml_system__frametable, .-caml_system__frametable
+        OBJECT G(caml_system__frametable)
 
+#if !defined(SYS_macosx)
 /* Mark stack as non-executable */
         .section .note.GNU-stack,"",%progbits
+#endif
diff --git a/asmrun/signals_osdep.h b/asmrun/signals_osdep.h
index d9bc8b18a..89d96933e 100644
--- a/asmrun/signals_osdep.h
+++ b/asmrun/signals_osdep.h
@@ -46,8 +46,9 @@
   #include <sys/ucontext.h>
   #include <AvailabilityMacros.h>
 
-  #if !defined(MAC_OS_X_VERSION_10_5) \
-      || MAC_OS_X_VERSION_MIN_REQUIRED < MAC_OS_X_VERSION_10_5
+  #if (!defined(MAC_OS_X_VERSION_10_5)                            \
+       || MAC_OS_X_VERSION_MIN_REQUIRED < MAC_OS_X_VERSION_10_5)  \
+      && !defined(__IPHONE_OS_VERSION_MIN_REQUIRED)
     #define CONTEXT_REG(r) r
   #else
     #define CONTEXT_REG(r) __##r
@@ -218,8 +219,9 @@
   #include <sys/ucontext.h>
   #include <AvailabilityMacros.h>
 
-  #if !defined(MAC_OS_X_VERSION_10_5) \
-      || MAC_OS_X_VERSION_MIN_REQUIRED < MAC_OS_X_VERSION_10_5
+  #if (!defined(MAC_OS_X_VERSION_10_5)                            \
+       || MAC_OS_X_VERSION_MIN_REQUIRED < MAC_OS_X_VERSION_10_5)  \
+      && !defined(__IPHONE_OS_VERSION_MIN_REQUIRED)
     #define CONTEXT_REG(r) r
   #else
     #define CONTEXT_REG(r) __##r
diff --git a/byterun/floats.c b/byterun/floats.c
index 4d2494cfb..f50217831 100644
--- a/byterun/floats.c
+++ b/byterun/floats.c
@@ -566,7 +566,7 @@ CAMLprim value caml_log1p_float(value f)
 
 union double_as_two_int32 {
     double d;
-#if defined(ARCH_BIG_ENDIAN) || (defined(__arm__) && !defined(__ARM_EABI__))
+#if defined(ARCH_BIG_ENDIAN) || (defined(__arm__) && !defined(__ARM_EABI__) && !defined(__APPLE__))
     struct { uint32_t h; uint32_t l; } i;
 #else
     struct { uint32_t l; uint32_t h; } i;
diff --git a/byterun/hash.c b/byterun/hash.c
index f7d0d2223..9a48f6228 100644
--- a/byterun/hash.c
+++ b/byterun/hash.c
@@ -90,7 +90,7 @@ CAMLexport uint32_t caml_hash_mix_double(uint32_t hash, double d)
 {
   union {
     double d;
-#if defined(ARCH_BIG_ENDIAN) || (defined(__arm__) && !defined(__ARM_EABI__))
+#if defined(ARCH_BIG_ENDIAN) || (defined(__arm__) && !defined(__ARM_EABI__) && !defined(__APPLE__))
     struct { uint32_t h; uint32_t l; } i;
 #else
     struct { uint32_t l; uint32_t h; } i;
diff --git a/byterun/ints.c b/byterun/ints.c
index 76ae11d4d..775d289ca 100644
--- a/byterun/ints.c
+++ b/byterun/ints.c
@@ -604,7 +604,7 @@ int64_t caml_int64_bits_of_float_unboxed(double d)
 {
   union { double d; int64_t i; int32_t h[2]; } u;
   u.d = d;
-#if defined(__arm__) && !defined(__ARM_EABI__)
+#if defined(__arm__) && !defined(__ARM_EABI__) && !defined(__APPLE__)
   { int32_t t = u.h[0]; u.h[0] = u.h[1]; u.h[1] = t; }
 #endif
   return u.i;
@@ -614,7 +614,7 @@ double caml_int64_float_of_bits_unboxed(int64_t i)
 {
   union { double d; int64_t i; int32_t h[2]; } u;
   u.i = i;
-#if defined(__arm__) && !defined(__ARM_EABI__)
+#if defined(__arm__) && !defined(__ARM_EABI__) && !defined(__APPLE__)
   { int32_t t = u.h[0]; u.h[0] = u.h[1]; u.h[1] = t; }
 #endif
   return u.d;
